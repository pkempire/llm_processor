Metadata-Version: 2.1
Name: llm_processor
Version: 0.1.0
Summary: High-performance batch processing library for LLM operations
Home-page: https://github.com/yourusername/llm_processor
Author: Your Name
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: openai>=1.0.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: tiktoken>=0.5.0
Requires-Dist: requests>=2.28.0

HyperLLM

A high-performance Python library for batch processing and concurrency of LLM API calls, achieving up to 1000x faster speeds than sequential API calls.

Built for:
1. **High-volume offline data processing** – concurrency + checkpointing + caching → reduces multi-hour jobs to minutes.  
2. **Real-time request handling** – dynamic concurrency, load balancing, rate limiting → keep latencies low and handle spikes.  
3. **Agent or multi-step reasoning** – managing repeated LLM calls in a single user flow, or tool + LLM interplay.

**Examples are using DeepSeek API** as they have **No Rate Limit** (or adapt to your preferred LLM provider).

---

### Highlights

- **Concurrency**: Uses Python’s `ThreadPoolExecutor` to make thousands of LLM calls in parallel, significantly reducing total runtime.  
- **Batching**: Merge multiple items into a single LLM prompt to reduce API overhead costs like connection setup, TLS handshakes, HTTP request/response cycles, and network latency. Instead of paying these costs per item, our auto batching amortizes them across multiple items in one API call.
- **Caching**: Optionally cache and skip repeated prompts with on-disk JSON caching.  
- **Retry**: Automatic exponential backoff for failures or rate-limit responses.  
- **Dynamic Token Batching**: Automatically chunk items so total tokens stay below a configured limit.  
- **Configurable**: A simple `ProcessorConfig` dataclass to set concurrency, batch size, dynamic token usage, etc.

---

## Installation

1. **Clone the repo**:
   ```bash
   git clone https://github.com/yourname/my_llm_lib.git
   cd my_llm_lib
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
   *(Add any dependencies like `tqdm`, `requests`, or `tiktoken` if needed.)*

3. **Use it in your projects**:
   ```python
   from llm_processor import LLMProcessor, ProcessorConfig, DeepSeekClient

   # Example configuration
   config = ProcessorConfig(
       max_workers=16,
       batch_size=8,
       cache_enabled=True,
       max_retries=3
   )

   # Initialize client and processor
   client = DeepSeekClient(api_key="your_api_key_here")
   processor = LLMProcessor(client, config)
   ```

---

## Quick Start

```python
from llm_processor.processor import LLMProcessor
from llm_processor.processor_config import ProcessorConfig
from llm_processor.llm_client import BaseLLMClient

# 1) Define or import your LLM client
class FakeLLMClient(BaseLLMClient):
    def call_api(self, prompt: str, system_prompt=None, **kwargs):
        # Simulate an LLM call
        return {"content": "Simulated response", "success": True}

    def validate_response(self, response):
        return True

# 2) Create a config
config = ProcessorConfig(
    max_workers=10,
    batch_size=10,
    enable_batch_prompts=True,
    enable_dynamic_token_batching=False,
    # ...
)

# 3) Create the processor
client = FakeLLMClient()
processor = LLMProcessor(llm_client=client, config=config)

# 4) Provide a "process function"
def process_subbatch(batch_of_items):
    # Build a single prompt from multiple items
    prompt = "Combine these items:\n"
    for it in batch_of_items:
        prompt += f"- {it}\n"
    response = client.call_api(prompt)
    return {"batch_result": response}

# 5) Run
items = [f"Item {i}" for i in range(50)]
results = processor.process_batch(items, process_subbatch)
print(results)
```

---

## Dynamic Token Batching

If you want to ensure each sub-batch stays under a certain token limit:

```python
config = ProcessorConfig(
    enable_dynamic_token_batching=True,
    max_tokens_per_batch=2048,
    token_counter_fn=my_token_counter,  # or omit to use default
    # ...
)
```
Then call `processor.process_batch(...)` normally. The library will chunk your items automatically so each sub-batch is below 2048 tokens total.

---

## Contributing

1. Fork the repo and create your feature branch.
2. Add tests for your changes.
3. Submit a pull request.

---

## License

MIT License
