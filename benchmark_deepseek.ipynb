{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_processing_example.ipynb\n",
    "import pandas as pd\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Setup Configuration\n",
    "config = ProcessorConfig(\n",
    "    batch_size=1,                    # Process 10 videos at a time\n",
    "    max_workers=100,                  # High concurrency for DeepSeek\n",
    "    cache_dir=\"sponsor_cache\",        # Cache directory\n",
    "    save_interval=5,                  # Save every 5 batches\n",
    "    show_progress=True,\n",
    "    metrics_output_path=\"metrics.json\"\n",
    ")\n",
    "\n",
    "# 2. Initialize DeepSeek Client\n",
    "client = DeepSeekClient(\n",
    "    api_key='sk-cd405682db094b6781f9f815840163d8',\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 3. Initialize Processor\n",
    "processor = LLMProcessor(client, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_processing.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VideoData:\n",
    "    \"\"\"Structure for video data\"\"\"\n",
    "    video_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    channel_id: Optional[str] = None\n",
    "    channel_title: Optional[str] = None\n",
    "\n",
    "def create_prompt(videos: List[Dict[str, Any]], desc_length: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Create prompt for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video data dictionaries\n",
    "        desc_length: Max length for description truncation\n",
    "    \"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(videos, 1):\n",
    "        description = video['description']\n",
    "        if len(description) > desc_length:\n",
    "            description = description[:desc_length] + \"...\"\n",
    "            \n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(videos)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content: str) -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Process LLM response into structured sponsor data\n",
    "    \n",
    "    Args:\n",
    "        content: Raw LLM response text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping video IDs to sponsor lists\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean up response if it contains markdown code blocks\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        print(f\"Raw content: {content[:200]}...\")  # Print start of content for debugging\n",
    "        return {}\n",
    "\n",
    "def process_video_batch(batch: Dict[str, Any], client) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single video batch for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing video data\n",
    "        client: LLM client instance\n",
    "    \"\"\"\n",
    "    # Create single-item batch for prompt\n",
    "    batch_list = [batch]\n",
    "    \n",
    "    # Generate and call prompt\n",
    "    prompt = create_prompt(batch_list)\n",
    "    response = client.call_api(prompt)\n",
    "    \n",
    "    # Return structured result\n",
    "    result = {\n",
    "        'video_id': batch['videoId'],\n",
    "        'processed_data': response\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def transform_results(result: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Transform LLM results into structured sponsor records\n",
    "    \n",
    "    Args:\n",
    "        result: Dictionary containing video ID and processed data\n",
    "        \n",
    "    Returns:\n",
    "        List of sponsor records with normalized structure\n",
    "    \"\"\"\n",
    "    video_id = result['video_id']\n",
    "    processed_data = result['processed_data']\n",
    "    \n",
    "    if not processed_data.get('success'):\n",
    "        print(f\"Processing failed for video {video_id}: {processed_data.get('error')}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse sponsors from LLM response\n",
    "    sponsor_data = process_batch_response(processed_data['content'])\n",
    "    sponsors = sponsor_data.get(video_id, [])\n",
    "    \n",
    "    # Create individual records for each sponsor\n",
    "    records = []\n",
    "    for i, sponsor in enumerate(sponsors, 1):\n",
    "        record = {\n",
    "            'video_id': video_id,\n",
    "            f'sponsor_{i}_name': sponsor.get('name'),\n",
    "            f'sponsor_{i}_domain': sponsor.get('domain'),\n",
    "            f'sponsor_{i}_evidence': sponsor.get('evidence')\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "def validate_sponsor_record(record: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate a sponsor record\n",
    "    \n",
    "    Args:\n",
    "        record: Dictionary containing sponsor data\n",
    "        \n",
    "    Returns:\n",
    "        True if record is valid, False otherwise\n",
    "    \"\"\"\n",
    "    required_fields = ['video_id']\n",
    "    sponsor_fields = ['name', 'domain', 'evidence']\n",
    "    \n",
    "    # Check required fields\n",
    "    if not all(field in record for field in required_fields):\n",
    "        return False\n",
    "        \n",
    "    # Check that at least one sponsor exists\n",
    "    has_sponsor = False\n",
    "    i = 1\n",
    "    while f'sponsor_{i}_name' in record:\n",
    "        sponsor_valid = all(\n",
    "            record.get(f'sponsor_{i}_{field}') \n",
    "            for field in sponsor_fields\n",
    "        )\n",
    "        if sponsor_valid:\n",
    "            has_sponsor = True\n",
    "        i += 1\n",
    "        \n",
    "    return has_sponsor\n",
    "\n",
    "def process_sponsor_batch(\n",
    "    videos: List[Dict[str, Any]],\n",
    "    processor,\n",
    "    cache_prefix: str = \"sponsor_detection\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a batch of videos for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video data dictionaries\n",
    "        processor: LLMProcessor instance\n",
    "        cache_prefix: Prefix for cache keys\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing processed sponsor data\n",
    "    \"\"\"\n",
    "    # Process videos through LLM processor\n",
    "    results = processor.process_batch(\n",
    "        items=videos,\n",
    "        process_fn=process_video_batch,\n",
    "        transform_fn=transform_results,\n",
    "        cache_prefix=cache_prefix\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add metadata\n",
    "    df['processed_timestamp'] = pd.Timestamp.now()\n",
    "    df['cache_prefix'] = cache_prefix\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import concurrent.futures\n",
    "from time import perf_counter\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Global caches\n",
    "url_cache = {}\n",
    "domain_cache = {}\n",
    "first_pass_results = {}  # Cache for videos that found sponsors in first pass\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure function execution time\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = perf_counter()\n",
    "        wrapper.total_time += end - start\n",
    "        wrapper.calls += 1\n",
    "        return result\n",
    "    wrapper.total_time = 0\n",
    "    wrapper.calls = 0\n",
    "    return wrapper\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a requests session with retries and timeouts\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "@measure_time\n",
    "def expand_url(url):\n",
    "    \"\"\"Expand shortened URLs with robust caching and handling\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "        \n",
    "    if url in url_cache:\n",
    "        return url_cache[url]\n",
    "        \n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "            \n",
    "        session = create_session()\n",
    "        \n",
    "        # Special handling for known URL shorteners\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        if any(service in domain for service in ['bit.ly', 'goo.gl', 'tinyurl']):\n",
    "            response = session.get(\n",
    "                url,\n",
    "                allow_redirects=True,\n",
    "                timeout=10,\n",
    "                headers={'User-Agent': 'Mozilla/5.0'},\n",
    "                stream=True\n",
    "            )\n",
    "        else:\n",
    "            response = session.head(\n",
    "                url,\n",
    "                allow_redirects=True,\n",
    "                timeout=5\n",
    "            )\n",
    "        \n",
    "        final_url = response.url\n",
    "        if isinstance(response, requests.models.Response):\n",
    "            response.close()\n",
    "        \n",
    "        url_cache[url] = final_url\n",
    "        return final_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"URL expansion error for {url}: {e}\")\n",
    "        return url\n",
    "\n",
    "def quick_domain_extract(url):\n",
    "    \"\"\"Extract domain with improved caching\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    if url_lower in domain_cache:\n",
    "        return domain_cache[url_lower]\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url_lower)\n",
    "        domain = parsed.netloc or parsed.path.split('/')[0]\n",
    "        \n",
    "        for prefix in ['www.']:\n",
    "            if domain.startswith(prefix):\n",
    "                domain = domain[len(prefix):]\n",
    "                \n",
    "        domain_cache[url_lower] = domain\n",
    "        return domain\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"Create unified prompt for both passes\"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, row in enumerate(video_batch.iterrows(), 1):\n",
    "        _, video = row\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content):\n",
    "    \"\"\"Process LLM response without URL expansion\"\"\"\n",
    "    try:\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_batch(batch, is_second_pass=False):\n",
    "    \"\"\"Process one batch with two-pass system\"\"\"\n",
    "    try:\n",
    "        # Skip videos that already have sponsors from first pass\n",
    "        if is_second_pass:\n",
    "            batch = batch[~batch['videoId'].isin(first_pass_results.keys())]\n",
    "            if batch.empty:\n",
    "                return {}\n",
    "        \n",
    "        # Use appropriate description length\n",
    "        desc_length = 1500 if is_second_pass else 200\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": create_prompt(batch, desc_length)\n",
    "            }],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        batch_results = process_batch_response(content)\n",
    "        \n",
    "        # Cache first pass results\n",
    "        if not is_second_pass:\n",
    "            first_pass_results.update(batch_results)\n",
    "        \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {'second' if is_second_pass else 'first'} pass: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_videos_parallel(df, batch_size=5, max_workers=3):\n",
    "    \"\"\"Process videos with two-pass system\"\"\"\n",
    "    sponsor_map = {}\n",
    "    batches = [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Processing videos (first pass)\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # First pass - 200 tokens\n",
    "            futures = [executor.submit(process_batch, batch, False) for batch in batches]\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    sponsor_map.update(results)\n",
    "                    pbar.update(batch_size)\n",
    "                except Exception as e:\n",
    "                    print(f\"Batch processing failed: {e}\")\n",
    "    \n",
    "    # Second pass for videos without sponsors\n",
    "    remaining_videos = len(df) - len(first_pass_results)\n",
    "    if remaining_videos > 0:\n",
    "        with tqdm(total=remaining_videos, desc=\"Processing videos (second pass)\") as pbar:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                futures = [executor.submit(process_batch, batch, True) for batch in batches]\n",
    "                \n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    try:\n",
    "                        results = future.result()\n",
    "                        sponsor_map.update(results)\n",
    "                        pbar.update(len(results))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Batch processing failed: {e}\")\n",
    "    \n",
    "    return sponsor_map\n",
    "\n",
    "def expand_sponsor_data(df, sponsor_map):\n",
    "    \"\"\"Expand sponsor data into columns efficiently\"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find max sponsors accounting for all videos\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for all possible sponsors\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def save_checkpoint(df, sponsor_map, current_idx, pass_number):\n",
    "    \"\"\"Save processing checkpoint with pass information\"\"\"\n",
    "    temp_df = df.copy()\n",
    "    temp_df['sponsor_data'] = temp_df['videoId'].map(lambda x: json.dumps(sponsor_map.get(x, [])))\n",
    "    temp_df.to_csv(f'sponsors_checkpoint_pass{pass_number}_{current_idx}.csv', index=False)\n",
    "\n",
    "def print_stats(df, sponsor_map):\n",
    "    \"\"\"Print detailed processing statistics\"\"\"\n",
    "    first_pass_count = len(first_pass_results)\n",
    "    second_pass_count = len(sponsor_map) - first_pass_count\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    print(f\"Videos with sponsors found in first pass (200 tokens): {first_pass_count}\")\n",
    "    print(f\"Additional sponsors found in second pass (1500 tokens): {second_pass_count}\")\n",
    "    print(f\"Total videos with sponsors: {len(sponsor_map)}\")\n",
    "    \n",
    "    # URL expansion stats\n",
    "    if expand_url.calls > 0:\n",
    "        avg_time = expand_url.total_time / expand_url.calls\n",
    "        print(f\"\\nURL Processing:\")\n",
    "        print(f\"Total URLs processed: {expand_url.calls}\")\n",
    "        print(f\"Average processing time: {avg_time:.2f}s per URL\")\n",
    "        print(f\"Cache hits: {len(url_cache)}\")\n",
    "    \n",
    "    # Sponsor distribution\n",
    "    sponsor_counts = [len(sponsors) for sponsors in sponsor_map.values()]\n",
    "    if sponsor_counts:\n",
    "        print(\"\\nSponsor Distribution:\")\n",
    "        print(f\"Average sponsors per video: {sum(sponsor_counts)/len(sponsor_counts):.2f}\")\n",
    "        print(f\"Max sponsors in a video: {max(sponsor_counts)}\")\n",
    "        \n",
    "        count_distribution = pd.Series(sponsor_counts).value_counts().sort_index()\n",
    "        print(\"\\nVideos by sponsor count:\")\n",
    "        for count, videos in count_distribution.items():\n",
    "            print(f\"{count} sponsor(s): {videos} videos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read sample data\n",
    "        df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "        \n",
    "        # Process videos with two-pass system\n",
    "        sponsor_map = process_videos_parallel(\n",
    "            df, \n",
    "            batch_size=10,  # Adjust based on your testing\n",
    "            max_workers=100   # Adjust based on your CPU\n",
    "        )\n",
    "        \n",
    "        # Expand and save sponsor data\n",
    "        final_df = expand_sponsor_data(df, sponsor_map)\n",
    "        final_df.to_csv('sponsors_chunk_8.csv', index=False)\n",
    "        \n",
    "        # Print detailed stats\n",
    "        print_stats(df, sponsor_map)\n",
    "        \n",
    "        # Print sample of found sponsors\n",
    "        print(\"\\nSample Sponsors Found:\")\n",
    "        sample_videos = list(sponsor_map.items())[:3]\n",
    "        for video_id, sponsors in sample_videos:\n",
    "            print(f\"\\nVideo {video_id}:\")\n",
    "            for i, sponsor in enumerate(sponsors, 1):\n",
    "                print(f\"  Sponsor {i}:\")\n",
    "                print(f\"    Name: {sponsor.get('name')}\")\n",
    "                print(f\"    Domain: {sponsor.get('domain')}\")\n",
    "                print(f\"    Evidence: {sponsor.get('evidence')[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/16678 [00:16<25:36:23,  5.53s/it, processed=3, success_rate=0.0%, errors=0, avg_time=4.29s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 288\u001b[0m\n\u001b[1;32m    285\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Process with library\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_videos_with_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msk-cd405682db094b6781f9f815840163d8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    294\u001b[0m final_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[29], line 178\u001b[0m, in \u001b[0;36mprocess_videos_with_library\u001b[0;34m(df, api_key)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Process first pass\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m first_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_first_pass\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Process second pass only for remaining videos\u001b[39;00m\n\u001b[1;32m    184\u001b[0m remaining \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    185\u001b[0m     v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m first_pass_results\n\u001b[1;32m    187\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/multi_llm/llm_processor/multi_processing/processor.py:185\u001b[0m, in \u001b[0;36mLLMProcessor.process_batch\u001b[0;34m(self, items, process_fn, transform_fn, cache_prefix, use_cache, output_path)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    179\u001b[0m     fn \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_with_retry,\n\u001b[1;32m    181\u001b[0m         process_fn\u001b[38;5;241m=\u001b[39mprocess_fn,\n\u001b[1;32m    182\u001b[0m         cache_prefix\u001b[38;5;241m=\u001b[39mcache_prefix,\n\u001b[1;32m    183\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache\n\u001b[1;32m    184\u001b[0m     )\n\u001b[0;32m--> 185\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(fn, batch_data))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Transform results if needed\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_fn:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sponsor_processor.py\n",
    "\n",
    "from multi_processing.processor import LLMProcessor, ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"Create unified prompt for both passes\"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(video_batch, 1):\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content):\n",
    "    \"\"\"Process LLM response without URL expansion\"\"\"\n",
    "    try:\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return {}\n",
    "\n",
    "def expand_sponsor_data(df, sponsor_map):\n",
    "    \"\"\"Expand sponsor data into columns efficiently\"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find max sponsors accounting for all videos\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for all possible sponsors\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def print_stats(df, sponsor_map):\n",
    "    \"\"\"Print detailed processing statistics\"\"\"\n",
    "    first_pass_count = len(first_pass_results)\n",
    "    second_pass_count = len(sponsor_map) - first_pass_count\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    print(f\"Videos with sponsors found in first pass (200 tokens): {first_pass_count}\")\n",
    "    print(f\"Additional sponsors found in second pass (1500 tokens): {second_pass_count}\")\n",
    "    print(f\"Total videos with sponsors: {len(sponsor_map)}\")\n",
    "    \n",
    "    # Sponsor distribution\n",
    "    sponsor_counts = [len(sponsors) for sponsors in sponsor_map.values()]\n",
    "    if sponsor_counts:\n",
    "        print(\"\\nSponsor Distribution:\")\n",
    "        print(f\"Average sponsors per video: {sum(sponsor_counts)/len(sponsor_counts):.2f}\")\n",
    "        print(f\"Max sponsors in a video: {max(sponsor_counts)}\")\n",
    "        \n",
    "        count_distribution = pd.Series(sponsor_counts).value_counts().sort_index()\n",
    "        print(\"\\nVideos by sponsor count:\")\n",
    "        for count, videos in count_distribution.items():\n",
    "            print(f\"{count} sponsor(s): {videos} videos\")\n",
    "\n",
    "# New wrapper for library integration\n",
    "def process_videos_with_library(df: pd.DataFrame, api_key: str):\n",
    "    \"\"\"Process videos using library's built-in parallel processing\"\"\"\n",
    "    \n",
    "    # Initialize basic config\n",
    "    config = ProcessorConfig(\n",
    "    cache_enabled=False,     # No disk cache\n",
    "    rate_limit=0.0,          # No forced sleep between calls\n",
    "    max_retries=1,           # Or 2, if you rarely fail\n",
    "    batch_size=1,            # Not crucial if you're doing item-level concurrency anyway\n",
    "    max_workers=100,         # If your system can handle it\n",
    "    fail_fast=True,\n",
    "    # ... etc.\n",
    ")\n",
    "    \n",
    "    client = DeepSeekClient(api_key=api_key)\n",
    "    processor = LLMProcessor(client, config)\n",
    "    \n",
    "    # Track first pass results\n",
    "    first_pass_results = {}\n",
    "    \n",
    "    # Define processing functions\n",
    "    def process_first_pass(video):\n",
    "        \"\"\"First pass with short descriptions\"\"\"\n",
    "        response = client.call_api(\n",
    "            create_prompt([video], desc_length=200)\n",
    "        )\n",
    "        if response['success']:\n",
    "            results = process_batch_response(response['content'])\n",
    "            first_pass_results.update(results)\n",
    "        return results\n",
    "\n",
    "    # Process first pass\n",
    "    first_results = processor.process_batch(\n",
    "        items=df.to_dict('records'),\n",
    "        process_fn=process_first_pass\n",
    "    )\n",
    "    \n",
    "    # Process second pass only for remaining videos\n",
    "    remaining = [\n",
    "        v for v in df.to_dict('records')\n",
    "        if v['videoId'] not in first_pass_results\n",
    "    ]\n",
    "    \n",
    "    def process_second_pass(video):\n",
    "        \"\"\"Second pass with longer descriptions\"\"\"\n",
    "        response = client.call_api(\n",
    "            create_prompt([video], desc_length=1500)\n",
    "        )\n",
    "        if response['success']:\n",
    "            return process_batch_response(response['content'])\n",
    "        return {}\n",
    "\n",
    "    second_results = processor.process_batch(\n",
    "        items=remaining,\n",
    "        process_fn=process_second_pass\n",
    "    )\n",
    "    \n",
    "    # Combine results and expand\n",
    "    all_results = {**first_results, **second_results}\n",
    "    return expand_sponsor_data(df, all_results)\n",
    "\n",
    "    \n",
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "\n",
    "class Telemetry:\n",
    "    def __init__(self):\n",
    "        self.timings = defaultdict(list)\n",
    "        self.counts = defaultdict(int)\n",
    "        \n",
    "    def measure(self, operation):\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start = perf_counter()\n",
    "                result = func(*args, **kwargs)\n",
    "                duration = perf_counter() - start\n",
    "                self.timings[operation].append(duration)\n",
    "                self.counts[operation] += 1\n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def print_stats(self):\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        for op, times in self.timings.items():\n",
    "            avg_time = sum(times) / len(times)\n",
    "            total_time = sum(times)\n",
    "            print(f\"\\n{op}:\")\n",
    "            print(f\"  Count: {self.counts[op]}\")\n",
    "            print(f\"  Average time: {avg_time:.2f}s\")\n",
    "            print(f\"  Total time: {total_time:.2f}s\")\n",
    "            print(f\"  % of total time: {(total_time/sum(sum(t) for t in self.timings.values()))*100:.1f}%\")\n",
    "\n",
    "# Add to your processor\n",
    "telemetry = Telemetry()\n",
    "\n",
    "@telemetry.measure(\"API Call\")\n",
    "def process_batch(batch, is_second_pass=False):\n",
    "    \"\"\"Process batch with timing\"\"\"\n",
    "    try:\n",
    "        if is_second_pass:\n",
    "            batch = [v for v in batch if v['videoId'] not in first_pass_results]\n",
    "            if not batch:\n",
    "                return {}\n",
    "        \n",
    "        desc_length = 1500 if is_second_pass else 200\n",
    "        \n",
    "        # Measure prompt creation\n",
    "        start = perf_counter()\n",
    "        prompt = create_prompt(batch, desc_length)\n",
    "        telemetry.timings[\"prompt_creation\"].append(perf_counter() - start)\n",
    "        \n",
    "        # Measure API call\n",
    "        start = perf_counter()\n",
    "        response = client.call_api(prompt)\n",
    "        telemetry.timings[\"pure_api_call\"].append(perf_counter() - start)\n",
    "        \n",
    "        if not response['success']:\n",
    "            return {}\n",
    "            \n",
    "        # Measure response processing    \n",
    "        start = perf_counter()\n",
    "        batch_results = process_batch_response(response['content'])\n",
    "        telemetry.timings[\"response_processing\"].append(perf_counter() - start)\n",
    "        \n",
    "        if not is_second_pass:\n",
    "            first_pass_results.update(batch_results)\n",
    "            \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {'second' if is_second_pass else 'first'} pass: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read data\n",
    "        df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "        \n",
    "        # Process with library\n",
    "        final_df = process_videos_with_library(\n",
    "            df,\n",
    "            api_key='sk-cd405682db094b6781f9f815840163d8'\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        final_df.to_csv('sponsor_results.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error running sponsor processing: ProcessorConfig.__init__() got an unexpected keyword argument 'enable_dynamic_token_batching'\n"
     ]
    }
   ],
   "source": [
    "# sponsor_processor.py\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#########################################\n",
    "# Helper Functions\n",
    "#########################################\n",
    "\n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"\n",
    "    Create a unified prompt for sponsor extraction.\n",
    "    Each item in video_batch is a dict with keys: videoId, title, description, etc.\n",
    "    \"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(video_batch, 1):\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Parse the JSON returned by the LLM and extract a dict:\n",
    "    { video_id: [ {name, domain, evidence}, ... ] }\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # If the LLM wraps JSON in code fences, strip them out\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        #print(f\"Parsed JSON: {result}\")  # Debug print\n",
    "\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response JSON: {e}, Content: {content}\")\n",
    "        return {}\n",
    "\n",
    "def expand_sponsor_data(df: pd.DataFrame, sponsor_map: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add columns for each sponsor (name, domain, evidence) to the original DataFrame.\n",
    "    \"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find the maximum number of sponsors that any video has\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        # For each sponsor slot, create columns\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for the sponsor slots\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with the original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    return final_df\n",
    "\n",
    "#########################################\n",
    "# Two-Pass Sponsor Processing\n",
    "#########################################\n",
    "\n",
    "def process_videos_with_library(df: pd.DataFrame, api_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Two-pass sponsor extraction using the LLMProcessor\"\"\"\n",
    "\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        enable_batch_prompts=True,  # Enable batching\n",
    "        batch_size=10,             # Process 10 videos at a time\n",
    "        max_workers=1000,\n",
    "        fail_fast=True,\n",
    "        enable_dynamic_token_batching=True,\n",
    "        dynamic_rate_limit=7000\n",
    "    )\n",
    "    \n",
    "    client = DeepSeekClient(api_key=api_key, model=\"deepseek-chat\")\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    # Track all results across both passes\n",
    "    all_sponsors = {}\n",
    "    first_pass_sponsors = {}\n",
    "\n",
    "    def process_first_pass(batch: List[Dict]) -> Dict:\n",
    "        \"\"\"Process a batch of videos with short descriptions\"\"\"\n",
    "        prompt = create_prompt(batch, desc_length=200)\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        if not response.get('success'):\n",
    "            return {}\n",
    "            \n",
    "        sponsors = process_batch_response(response['content'])\n",
    "        # Update our tracking dict\n",
    "        first_pass_sponsors.update(sponsors)\n",
    "        return sponsors\n",
    "\n",
    "    # First pass\n",
    "    print(\"\\nRunning first pass...\")\n",
    "    first_pass_results = processor.process_batch(\n",
    "        items=df.to_dict('records'),\n",
    "        process_fn=process_first_pass,\n",
    "        desc=\"First pass (200 tokens)\"\n",
    "    )\n",
    "    \n",
    "    # Combine first pass results\n",
    "    for result in first_pass_results:\n",
    "        all_sponsors.update(result)\n",
    "\n",
    "    # Second pass only for videos without sponsors\n",
    "    remaining = [\n",
    "        video for video in df.to_dict('records')\n",
    "        if video['videoId'] not in first_pass_sponsors\n",
    "    ]\n",
    "\n",
    "    if remaining:\n",
    "        print(f\"\\nRunning second pass for {len(remaining)} videos...\")\n",
    "        \n",
    "        def process_second_pass(batch: List[Dict]) -> Dict:\n",
    "            \"\"\"Process a batch with longer descriptions\"\"\"\n",
    "            prompt = create_prompt(batch, desc_length=1500)\n",
    "            response = client.call_api(prompt)\n",
    "            \n",
    "            if not response.get('success'):\n",
    "                return {}\n",
    "                \n",
    "            return process_batch_response(response['content'])\n",
    "\n",
    "        second_pass_results = processor.process_batch(\n",
    "            items=remaining,\n",
    "            process_fn=process_second_pass,\n",
    "            desc=\"Second pass (1500 tokens)\"\n",
    "        )\n",
    "        \n",
    "        # Combine second pass results\n",
    "        for result in second_pass_results:\n",
    "            all_sponsors.update(result)\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total videos: {len(df)}\")\n",
    "    print(f\"First pass sponsors: {len(first_pass_sponsors)}\")\n",
    "    print(f\"Total sponsors found: {len(all_sponsors)}\")\n",
    "\n",
    "    # Expand and return\n",
    "    return expand_sponsor_data(df, all_sponsors)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Example Main\n",
    "#########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load your CSV\n",
    "        df = pd.read_csv(\"/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv\", nrows=1000)\n",
    "        \n",
    "        # Put your actual DeepSeek/LLM API key here\n",
    "        API_KEY = \"sk-cd405682db094b6781f9f815840163d8\"\n",
    "\n",
    "        # Run the two-pass sponsor extraction\n",
    "        final_df = process_videos_with_library(df, API_KEY)\n",
    "\n",
    "        # Save results\n",
    "        final_df.to_csv(\"sponsor_results.csv\", index=False)\n",
    "        \n",
    "        print(\"Processing complete! Results saved to sponsor_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running sponsor processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_video_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m video_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process videos with our framework\u001b[39;00m\n\u001b[1;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mprocess_batch(\n\u001b[1;32m     10\u001b[0m     items\u001b[38;5;241m=\u001b[39mvideo_data,\n\u001b[0;32m---> 11\u001b[0m     process_fn\u001b[38;5;241m=\u001b[39m\u001b[43mprocess_video_batch\u001b[49m,\n\u001b[1;32m     12\u001b[0m     transform_fn\u001b[38;5;241m=\u001b[39mtransform_results,\n\u001b[1;32m     13\u001b[0m     cache_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_extraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 6. Analyze Results\u001b[39;00m\n\u001b[1;32m     18\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_video_batch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Load and Process Data\n",
    "# Load your video data\n",
    "df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "\n",
    "# Convert DataFrame rows to list of dicts\n",
    "video_data = df.to_dict('records')\n",
    "\n",
    "# Process videos with our framework\n",
    "results = processor.process_batch(\n",
    "    items=video_data,\n",
    "    process_fn=process_video_batch,\n",
    "    transform_fn=transform_results,\n",
    "    cache_prefix='sponsor_extraction',\n",
    "    output_path='sponsor_results.csv'\n",
    ")\n",
    "\n",
    "# 6. Analyze Results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nProcessing Statistics:\")\n",
    "print(f\"Total videos processed: {len(df)}\")\n",
    "print(f\"Videos with sponsors found: {len(results_df)}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample Sponsor Results:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# 7. Check Processing Metrics\n",
    "import json\n",
    "with open(\"metrics.json\", 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"\\nProcessing Metrics:\")\n",
    "print(f\"Total processing time: {metrics['total_time']:.2f} seconds\")\n",
    "print(f\"Average time per item: {metrics['avg_process_time']:.2f} seconds\")\n",
    "print(f\"Cache hits: {metrics['cache_hits']}\")\n",
    "print(f\"Total errors: {metrics['errors']}\")\n",
    "\n",
    "# 8. Additional Analysis\n",
    "if results_df.empty:\n",
    "    print(\"No results found\")\n",
    "else:\n",
    "    # Get sponsor frequency\n",
    "    sponsor_cols = [col for col in results_df.columns if 'sponsor_' in col and 'name' in col]\n",
    "    all_sponsors = results_df[sponsor_cols].values.flatten()\n",
    "    sponsor_counts = pd.Series(all_sponsors).value_counts().dropna()\n",
    "\n",
    "    print(\"\\nTop Sponsors:\")\n",
    "    print(sponsor_counts.head())\n",
    "\n",
    "# 9. Save Final Results\n",
    "results_df.to_csv('final_sponsor_analysis.csv', index=False)\n",
    "print(\"\\nResults saved to 'final_sponsor_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 200/200 [00:04<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library] Processed 200 items in 4.24 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw concurrency: 100%|| 200/200 [00:04<00:00, 47.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Concurrency] Processed 200 items in 4.21 seconds.\n",
      "Library results: 200 items.\n",
      "Raw concurrency results: 200 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_processor_speed.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# 1) Fake LLMClient to Simulate API Calls\n",
    "########################################\n",
    "\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class FakeLLMClient(BaseLLMClient):\n",
    "    \"\"\"\n",
    "    A fake client that simulates a 100-300ms \"API call\" time.\n",
    "    No real network usage, just time.sleep().\n",
    "    \"\"\"\n",
    "\n",
    "    def call_api(self, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        # Sleep a random time to simulate latency\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        # Return a pretend success payload\n",
    "        return {\n",
    "            \"content\": f\"Fake response for prompt: {prompt[:30]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def validate_response(self, response: Dict[str, Any]) -> bool:\n",
    "        return response.get(\"success\", False)\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "\n",
    "def generate_fake_dataset(num_items: int = 2000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random text data.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=100))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) The \"process function\" we apply\n",
    "########################################\n",
    "\n",
    "def process_item_with_fake_llm(item: Dict[str, Any], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a single item, call the fake LLM API and return a small result dict.\n",
    "    \"\"\"\n",
    "    prompt = f\"Process text: {item['text']}\"\n",
    "    response = client.call_api(prompt)\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"content\": response[\"content\"],  # from the fake LLM\n",
    "        \"success\": response[\"success\"]\n",
    "    }\n",
    "\n",
    "def run_sequential_no_concurrency(dataset, client):\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Process text: {item['text']}\"\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"content\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        })\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(results)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "########################################\n",
    "# 4) Testing with the LLMProcessor Library\n",
    "########################################\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "def run_with_library(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use your LLMProcessor with concurrency, no caching, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,  # no disk caching\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=1,         # item-level concurrency\n",
    "        fail_fast=False\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()  # Our fake client\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        return process_item_with_fake_llm(item, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    results = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\",   # not used if cache is disabled\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Library] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 5) Testing with Raw Concurrency (ThreadPool)\n",
    "########################################\n",
    "\n",
    "def run_with_raw_concurrency(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use a plain ThreadPoolExecutor approach, no library overhead.\n",
    "    \"\"\"\n",
    "    client = FakeLLMClient()\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=\"Raw concurrency\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in dataset:\n",
    "                futures.append(executor.submit(process_item_with_fake_llm, item, client))\n",
    "            \n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                res = f.result()\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Raw Concurrency] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 6) Main Comparison\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate a sample dataset\n",
    "    data_size = 200  # adjust to see bigger difference\n",
    "    dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "    # 2) Run with library\n",
    "    library_results = run_with_library(dataset, max_workers=10)\n",
    "\n",
    "    # 3) Run with raw concurrency\n",
    "    raw_results = run_with_raw_concurrency(dataset, max_workers=10)\n",
    "\n",
    "    control_results = run_sequential_no_concurrency(dataset, client)\n",
    "\n",
    "    # 4) Quick check\n",
    "    # Validate that we got the same number of results\n",
    "    print(f\"Library results: {len(library_results)} items.\")\n",
    "    print(f\"Raw concurrency results: {len(raw_results)} items.\")\n",
    "    \n",
    "    # If you want to confirm the outputs are consistent, you can compare them\n",
    "    # but here we only compare time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 1/1 [00:00<00:00, 1498.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic processing test:\n",
      "Number of results: 1\n",
      "All successful: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 1/1 [00:00<00:00, 595.27it/s]\n",
      "Processing items: 100%|| 1/1 [00:00<00:00, 571.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching test:\n",
      "Number of calls after cache: 0\n",
      "Results match: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 3/3 [00:00<00:00, 3542.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch processing test:\n",
      "Number of batches: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 1/1 [00:00<00:00, 2501.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry logic test:\n",
      "Success after retries: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 2/2 [00:00<00:00, 18.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concurrent processing test:\n",
      "Processing time: 0.11s\n",
      "Number of results: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import unittest\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Import processor modules\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class MockLLMClient(BaseLLMClient):\n",
    "    \"\"\"Mock LLM client for testing\"\"\"\n",
    "    def __init__(self, delay=0.1):\n",
    "        self.calls = []\n",
    "        self.delay = delay\n",
    "        \n",
    "    def call_api(self, prompt: str, system_prompt=None, **kwargs):\n",
    "        self.calls.append({\"prompt\": prompt, \"system\": system_prompt})\n",
    "        return {\n",
    "            \"content\": f\"Response to: {prompt[:20]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    def validate_response(self, response):\n",
    "        return True\n",
    "\n",
    "# Set up test environment\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "config = ProcessorConfig(\n",
    "    batch_size=2,\n",
    "    max_workers=2, \n",
    "    cache_dir=os.path.join(temp_dir, \"cache\"),\n",
    "    metrics_output_path=os.path.join(temp_dir, \"metrics.json\"),\n",
    "    enable_batch_prompts=True,\n",
    "    cache_enabled=True,\n",
    "    max_retries=2\n",
    ")\n",
    "client = MockLLMClient()\n",
    "processor = LLMProcessor(client, config)\n",
    "\n",
    "# Test basic processing\n",
    "items = [\"test1\", \"test2\"]\n",
    "\n",
    "def process_fn(item):\n",
    "    return client.call_api(f\"Process: {item}\")\n",
    "    \n",
    "results = processor.process_batch(items, process_fn)\n",
    "print(\"Basic processing test:\")\n",
    "print(f\"Number of results: {len(results)}\")\n",
    "print(f\"All successful: {all(r['success'] for r in results)}\")\n",
    "\n",
    "# Test caching\n",
    "items = [\"cache_test\"]\n",
    "\n",
    "results1 = processor.process_batch(items, process_fn, use_cache=True)\n",
    "initial_calls = len(client.calls)\n",
    "\n",
    "results2 = processor.process_batch(items, process_fn, use_cache=True) \n",
    "print(\"\\nCaching test:\")\n",
    "print(f\"Number of calls after cache: {len(client.calls) - initial_calls}\")\n",
    "print(f\"Results match: {results1 == results2}\")\n",
    "\n",
    "# Test batch processing\n",
    "items = [f\"item{i}\" for i in range(5)]\n",
    "\n",
    "def batch_process_fn(batch):\n",
    "    return client.call_api(f\"Batch: {batch}\")\n",
    "    \n",
    "results = processor.process_batch(items, batch_process_fn)\n",
    "print(\"\\nBatch processing test:\")\n",
    "print(f\"Number of batches: {len(results)}\")\n",
    "\n",
    "# Test retry logic\n",
    "class FailingClient(MockLLMClient):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fail_count = 2\n",
    "        \n",
    "    def call_api(self, prompt, **kwargs):\n",
    "        if self.fail_count > 0:\n",
    "            self.fail_count -= 1\n",
    "            raise Exception(\"Simulated failure\")\n",
    "        return super().call_api(prompt, **kwargs)\n",
    "\n",
    "processor.llm_client = FailingClient()\n",
    "items = [\"retry_test\"]\n",
    "\n",
    "results = processor.process_batch(items, process_fn)\n",
    "print(\"\\nRetry logic test:\")\n",
    "print(f\"Success after retries: {results[0]['success']}\")\n",
    "\n",
    "# Test concurrent processing\n",
    "import time\n",
    "\n",
    "class SlowMockClient(MockLLMClient):\n",
    "    def call_api(self, prompt, **kwargs):\n",
    "        time.sleep(0.1)\n",
    "        return super().call_api(prompt, **kwargs)\n",
    "\n",
    "processor.llm_client = SlowMockClient()\n",
    "items = [f\"item{i}\" for i in range(4)]\n",
    "\n",
    "start = time.time()\n",
    "results = processor.process_batch(items, lambda x: processor.llm_client.call_api(f\"Process: {x}\"))\n",
    "duration = time.time() - start\n",
    "\n",
    "print(\"\\nConcurrent processing test:\")\n",
    "print(f\"Processing time: {duration:.2f}s\")\n",
    "print(f\"Number of results: {len(results)}\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential (control): 100%|| 500/500 [01:41<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Control] Processed 500 items sequentially in 101.50 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|| 500/500 [00:01<00:00, 416.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library Item-Level] Processed 500 items in 1.21 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Library Batch Mode: 100%|| 50/50 [00:00<00:00, 163.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library Batch Mode] Processed 500 items in 0.31 seconds (sub-batch size=10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw concurrency: 100%|| 500/500 [00:01<00:00, 412.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Concurrency] Processed 500 items in 1.22 seconds.\n",
      "\n",
      "Control results: 500 items.\n",
      "Library item-level results: 500 items.\n",
      "Library batch-mode results: 500 items.\n",
      "Raw concurrency results: 500 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_processor_speed.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# 1) Fake LLMClient to Simulate API Calls\n",
    "########################################\n",
    "\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class FakeLLMClient(BaseLLMClient):\n",
    "    \"\"\"\n",
    "    A fake client that simulates a 100-300ms \"API call\" time.\n",
    "    No real network usage, just time.sleep().\n",
    "    \"\"\"\n",
    "\n",
    "    def call_api(self, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        # Sleep a random time to simulate latency\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        # Return a pretend success payload\n",
    "        return {\n",
    "            \"content\": f\"Fake response for prompt: {prompt[:30]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def validate_response(self, response: Dict[str, Any]) -> bool:\n",
    "        return response.get(\"success\", False)\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "\n",
    "def generate_fake_dataset(num_items: int = 2000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random text data.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=100))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) The \"process function\" for item-level\n",
    "########################################\n",
    "\n",
    "def process_item_with_fake_llm(item: Dict[str, Any], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a single item, call the fake LLM API and return a small result dict.\n",
    "    \"\"\"\n",
    "    prompt = f\"Process text: {item['text']}\"\n",
    "    response = client.call_api(prompt)\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"content\": response[\"content\"],  # from the fake LLM\n",
    "        \"success\": response[\"success\"]\n",
    "    }\n",
    "\n",
    "########################################\n",
    "# 4) No concurrency: control group\n",
    "########################################\n",
    "\n",
    "def run_sequential_no_concurrency(dataset, client):\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Process text: {item['text']}\"\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"content\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        })\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(results)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 5) The Library with item-level concurrency\n",
    "########################################\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "def run_with_library(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use your LLMProcessor with concurrency, no caching, etc., item-level.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=1,         # item-level concurrency\n",
    "        fail_fast=False,\n",
    "        enable_batch_prompts=False  # <== ensure item-level\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        return process_item_with_fake_llm(item, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    results = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\", \n",
    "        use_cache=False\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Library Item-Level] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 6) The Library with \"Batch\" prompts\n",
    "########################################\n",
    "\n",
    "def create_subbatch_prompt(items: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Example: combine multiple items into a single prompt.\n",
    "    items might be up to config.batch_size in length.\n",
    "    \"\"\"\n",
    "    # Just build a single text listing them\n",
    "    combined_text = \"\"\n",
    "    for itm in items:\n",
    "        combined_text += f\"(ID={itm['id']}) {itm['text']}\\n\"\n",
    "    prompt = f\"Process these {len(items)} texts at once:\\n{combined_text}\"\n",
    "    return prompt\n",
    "\n",
    "def process_subbatch(subbatch: List[Dict[str, Any]], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Called once per sub-batch. We call the LLM once for all items in subbatch.\n",
    "    Return a dict with the results. For example, { item_id -> info }.\n",
    "    \"\"\"\n",
    "    prompt = create_subbatch_prompt(subbatch)\n",
    "    response = client.call_api(prompt)\n",
    "    \n",
    "    # We'll pretend we parse out something. For now, just store the prompt.\n",
    "    # In a real scenario, you'd parse JSON containing all items' results.\n",
    "    result_map = {}\n",
    "    for itm in subbatch:\n",
    "        result_map[itm['id']] = {\n",
    "            \"id\": itm[\"id\"],\n",
    "            \"text\": itm[\"text\"],\n",
    "            \"combined_response\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        }\n",
    "    return result_map\n",
    "\n",
    "def run_with_library_batch_mode(dataset: List[Dict[str, Any]], max_workers: int = 10, batch_size: int = 10):\n",
    "    \"\"\"\n",
    "    Concurrency across sub-batches (enable_batch_prompts=True).\n",
    "    Each sub-batch calls the LLM once for multiple items.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=batch_size,  # sub-batch size\n",
    "        fail_fast=False,\n",
    "        enable_batch_prompts=True  # <== batch mode\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(subbatch: List[Dict[str, Any]]) -> Dict[int, Any]:\n",
    "        return process_subbatch(subbatch, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    # This returns a list of dicts. Each dict is { item_id -> info } for one sub-batch\n",
    "    dict_list = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\", \n",
    "        use_cache=False,\n",
    "        desc=\"Library Batch Mode\"\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "\n",
    "    # Combine all subdicts\n",
    "    combined = {}\n",
    "    for subdict in dict_list:\n",
    "        combined.update(subdict)  # merges item_id -> info\n",
    "    \n",
    "    print(f\"[Library Batch Mode] Processed {len(combined)} items in {elapsed:.2f} seconds (sub-batch size={batch_size}).\")\n",
    "    return combined\n",
    "\n",
    "########################################\n",
    "# 7) Raw Concurrency (ThreadPool)\n",
    "########################################\n",
    "\n",
    "def run_with_raw_concurrency(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use a plain ThreadPoolExecutor approach, no library overhead.\n",
    "    \"\"\"\n",
    "    client = FakeLLMClient()\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=\"Raw concurrency\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in dataset:\n",
    "                futures.append(executor.submit(process_item_with_fake_llm, item, client))\n",
    "            \n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                res = f.result()\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Raw Concurrency] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 8) Main Comparison\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate a sample dataset\n",
    "    data_size = 500\n",
    "    dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "    # We'll re-use one FakeLLMClient for the control\n",
    "    control_client = FakeLLMClient()\n",
    "    \n",
    "    # 2) Control: No concurrency\n",
    "    control_results = run_sequential_no_concurrency(dataset, control_client)\n",
    "\n",
    "    # 3) Library, item-level concurrency\n",
    "    library_item_results = run_with_library(dataset, max_workers=100)\n",
    "\n",
    "    # 4) Library, batch mode concurrency\n",
    "    #    Each sub-batch processes multiple items in a single fake API call\n",
    "    library_batch_results = run_with_library_batch_mode(dataset, max_workers=100, batch_size=10)\n",
    "\n",
    "    # 5) Raw concurrency\n",
    "    raw_results = run_with_raw_concurrency(dataset, max_workers=100)\n",
    "\n",
    "    # 6) Print final comparisons\n",
    "    print(f\"\\nControl results: {len(control_results)} items.\")\n",
    "    print(f\"Library item-level results: {len(library_item_results)} items.\")\n",
    "    print(f\"Library batch-mode results: {len(library_batch_results)} items.\")\n",
    "    print(f\"Raw concurrency results: {len(raw_results)} items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 61390\n"
     ]
    }
   ],
   "source": [
    "data_size = 500\n",
    "dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "def count_tokens(items: list) -> int:\n",
    "    sum = 0\n",
    "    for item in items:\n",
    "        # Since dataset is a list of dictionaries, we can directly get the length of each item\n",
    "        sum += len(str(item))  # Convert item to string to get its length\n",
    "    return sum\n",
    "\n",
    "print(f\"Total tokens: {count_tokens(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "class OutputFormat(BaseModel):\n",
    "    \"\"\"Base class for structured LLM outputs\"\"\"\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm_response: str) -> 'OutputFormat':\n",
    "        \"\"\"Parse LLM response into structured format\"\"\"\n",
    "        try:\n",
    "            # Try parsing as direct JSON first\n",
    "            data = json.loads(llm_response)\n",
    "            return cls(**data)\n",
    "        except (json.JSONDecodeError, ValidationError) as e:\n",
    "            # Could add more fallback parsing strategies here\n",
    "            raise ValueError(f\"Failed to parse LLM output: {e}\")\n",
    "\n",
    "    def to_prompt(self) -> str:\n",
    "        \"\"\"Convert schema to prompt instructions\"\"\"\n",
    "        schema = self.schema()\n",
    "        fields = schema.get('properties', {})\n",
    "        \n",
    "        instructions = [\"Please provide output in the following JSON format:\"]\n",
    "        instructions.append(\"{\")\n",
    "        \n",
    "        for field_name, field_info in fields.items():\n",
    "            field_type = field_info.get('type', 'any')\n",
    "            description = field_info.get('description', '')\n",
    "            required = field_name in schema.get('required', [])\n",
    "            \n",
    "            instructions.append(f'  \"{field_name}\": {field_type}, // {description}{\"(required)\" if required else \"\"}')\n",
    "            \n",
    "        instructions.append(\"}\")\n",
    "        return \"\\n\".join(instructions)\n",
    "\n",
    "# Example Output Formats\n",
    "class SentimentAnalysis(OutputFormat):\n",
    "    sentiment: str = Field(..., description=\"Overall sentiment (positive/negative/neutral)\")\n",
    "    confidence: float = Field(..., description=\"Confidence score 0-1\", ge=0, le=1)\n",
    "    key_phrases: List[str] = Field(..., description=\"Key phrases that influenced sentiment\")\n",
    "    \n",
    "class EntityExtraction(OutputFormat):\n",
    "    entities: List[Dict[str, str]] = Field(..., description=\"List of extracted entities\")\n",
    "    relationships: List[Dict[str, str]] = Field([], description=\"Entity relationships\")\n",
    "\n",
    "class CodeAnalysis(OutputFormat):\n",
    "    complexity: int = Field(..., description=\"Estimated code complexity 1-10\")\n",
    "    issues: List[Dict[str, str]] = Field(..., description=\"Potential issues found\")\n",
    "    suggestions: List[str] = Field(..., description=\"Improvement suggestions\")\n",
    "\n",
    "# Structured output manager\n",
    "class StructuredOutputManager:\n",
    "    \"\"\"Manages structured outputs and prompting for LLM responses\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client):\n",
    "        self.llm_client = llm_client\n",
    "        \n",
    "    async def get_structured_output(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_format: type[OutputFormat],\n",
    "        max_retries: int = 3\n",
    "    ) -> OutputFormat:\n",
    "        \"\"\"Get structured output from LLM with retry logic\"\"\"\n",
    "        \n",
    "        # Add format instructions to prompt\n",
    "        format_instructions = output_format.to_prompt()\n",
    "        full_prompt = f\"{prompt}\\n\\n{format_instructions}\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = await self.llm_client.complete(full_prompt)\n",
    "                return output_format.from_llm(response)\n",
    "            except (ValueError, ValidationError) as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                # Could add reformatting/retry logic here\n",
    "        \n",
    "    async def batch_structured_output(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        output_format: type[OutputFormat],\n",
    "        max_concurrent: int = 5\n",
    "    ) -> List[OutputFormat]:\n",
    "        \"\"\"Process multiple prompts concurrently with structured output\"\"\"\n",
    "        results = []\n",
    "        # Implementation would use LLMProcessor for concurrent processing\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 187\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(results, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Agent-specific output formats\n",
    "class TaskDecomposition(OutputFormat):\n",
    "    subtasks: List[Dict[str, str]] = Field(..., description=\"List of subtasks\")\n",
    "    dependencies: List[Dict[str, List[str]]] = Field(..., description=\"Task dependencies\")\n",
    "    estimated_complexity: int = Field(..., description=\"Estimated complexity 1-10\")\n",
    "\n",
    "class TaskResult(OutputFormat):\n",
    "    task_id: str = Field(..., description=\"ID of the completed task\")\n",
    "    status: str = Field(..., description=\"Success/Failure/Partial\")\n",
    "    output: Dict[str, Any] = Field(..., description=\"Task output\")\n",
    "    next_steps: Optional[List[str]] = Field(None, description=\"Suggested next steps\")\n",
    "\n",
    "class AgentState(Enum):\n",
    "    PLANNING = \"planning\"\n",
    "    EXECUTING = \"executing\"\n",
    "    REVIEWING = \"reviewing\"\n",
    "    COMPLETE = \"complete\"\n",
    "    ERROR = \"error\"\n",
    "\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"Maintains context for agent workflow\"\"\"\n",
    "    task_id: str\n",
    "    state: AgentState\n",
    "    parent_task: Optional[str] = None\n",
    "    artifacts: Dict[str, Any] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class AgentWorkflow:\n",
    "    \"\"\"Implements concurrent agent workflow pattern\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_processor: LLMProcessor,\n",
    "        output_manager: StructuredOutputManager,\n",
    "        max_concurrent_tasks: int = 3\n",
    "    ):\n",
    "        self.llm_processor = llm_processor\n",
    "        self.output_manager = output_manager\n",
    "        self.max_concurrent = max_concurrent_tasks\n",
    "        self.contexts: Dict[str, AgentContext] = {}\n",
    "        \n",
    "    async def decompose_task(self, task_description: str) -> TaskDecomposition:\n",
    "        \"\"\"Break down complex task into subtasks\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following task and break it down into subtasks:\n",
    "        {task_description}\n",
    "        \n",
    "        Consider dependencies between subtasks and estimate complexity.\n",
    "        \"\"\"\n",
    "        return await self.output_manager.get_structured_output(\n",
    "            prompt,\n",
    "            TaskDecomposition\n",
    "        )\n",
    "        \n",
    "    async def execute_subtask(\n",
    "        self,\n",
    "        subtask: Dict[str, str],\n",
    "        context: AgentContext\n",
    "    ) -> TaskResult:\n",
    "        \"\"\"Execute individual subtask\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Execute the following subtask with given context:\n",
    "        Task: {subtask['description']}\n",
    "        Context: {context.metadata}\n",
    "        \n",
    "        Previous artifacts: {context.artifacts}\n",
    "        \"\"\"\n",
    "        return await self.output_manager.get_structured_output(\n",
    "            prompt,\n",
    "            TaskResult\n",
    "        )\n",
    "        \n",
    "    async def process_complex_task(\n",
    "        self,\n",
    "        task_description: str,\n",
    "        initial_context: Optional[Dict] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Process complex task with concurrent subtask execution\"\"\"\n",
    "        \n",
    "        # First decompose the task\n",
    "        decomposition = await self.decompose_task(task_description)\n",
    "        \n",
    "        # Create execution plan based on dependencies\n",
    "        execution_plan = self._create_execution_plan(decomposition)\n",
    "        \n",
    "        # Track task states\n",
    "        pending_tasks = set(execution_plan.keys())\n",
    "        completed_tasks = set()\n",
    "        failed_tasks = set()\n",
    "        results = {}\n",
    "        \n",
    "        while pending_tasks:\n",
    "            # Get tasks that can be executed (dependencies met)\n",
    "            available_tasks = [\n",
    "                task_id for task_id in pending_tasks\n",
    "                if all(dep in completed_tasks \n",
    "                      for dep in execution_plan[task_id]['dependencies'])\n",
    "            ]\n",
    "            \n",
    "            # Execute available tasks concurrently\n",
    "            tasks = []\n",
    "            for task_id in available_tasks[:self.max_concurrent]:\n",
    "                context = AgentContext(\n",
    "                    task_id=task_id,\n",
    "                    state=AgentState.EXECUTING,\n",
    "                    artifacts=results,\n",
    "                    metadata=initial_context or {}\n",
    "                )\n",
    "                self.contexts[task_id] = context\n",
    "                \n",
    "                task = self.execute_subtask(\n",
    "                    execution_plan[task_id]['task'],\n",
    "                    context\n",
    "                )\n",
    "                tasks.append(task)\n",
    "                \n",
    "            # Wait for batch completion\n",
    "            if tasks:\n",
    "                completed = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                \n",
    "                # Process results\n",
    "                for i, result in enumerate(completed):\n",
    "                    task_id = available_tasks[i]\n",
    "                    if isinstance(result, Exception):\n",
    "                        failed_tasks.add(task_id)\n",
    "                    else:\n",
    "                        completed_tasks.add(task_id)\n",
    "                        results[task_id] = result\n",
    "                    pending_tasks.remove(task_id)\n",
    "                    \n",
    "        # Final results analysis\n",
    "        return {\n",
    "            'results': results,\n",
    "            'failed_tasks': list(failed_tasks),\n",
    "            'execution_order': list(completed_tasks)\n",
    "        }\n",
    "        \n",
    "    def _create_execution_plan(\n",
    "        self,\n",
    "        decomposition: TaskDecomposition\n",
    "    ) -> Dict[str, Dict]:\n",
    "        \"\"\"Create execution plan from task decomposition\"\"\"\n",
    "        plan = {}\n",
    "        for i, task in enumerate(decomposition.subtasks):\n",
    "            task_id = f\"task_{i}\"\n",
    "            plan[task_id] = {\n",
    "                'task': task,\n",
    "                'dependencies': []\n",
    "            }\n",
    "            \n",
    "        # Add dependencies\n",
    "        for dep in decomposition.dependencies:\n",
    "            task_id = dep['task']\n",
    "            plan[task_id]['dependencies'] = dep['depends_on']\n",
    "            \n",
    "        return plan\n",
    "\n",
    "# Example usage:\n",
    "async def main():\n",
    "    # Setup\n",
    "    llm_client = BaseLLMClient()  # Your LLM implementation\n",
    "    llm_processor = LLMProcessor(llm_client)\n",
    "    output_manager = StructuredOutputManager(llm_client)\n",
    "    \n",
    "    workflow = AgentWorkflow(llm_processor, output_manager)\n",
    "    \n",
    "    # Complex task example\n",
    "    task = \"\"\"\n",
    "    Analyze the provided codebase:\n",
    "    1. Identify potential security vulnerabilities\n",
    "    2. Suggest performance improvements\n",
    "    3. Create documentation outline\n",
    "    4. List technical debt items\n",
    "    \"\"\"\n",
    "    \n",
    "    results = await workflow.process_complex_task(task)\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset of 50 items, ~5312 tokens total.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential (control): 100%|| 50/50 [00:51<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Control] Processed 50 items sequentially in 51.43 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Item-level concurrency (500 workers): 100%|| 50/50 [00:03<00:00, 15.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Item-Level Concurrency] Processed 50 items in 3.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ProcessorConfig.__init__() got an unexpected keyword argument 'enable_dynamic_token_batching'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 212\u001b[0m\n\u001b[1;32m    208\u001b[0m item_time \u001b[38;5;241m=\u001b[39m run_item_concurrency(dataset, max_workers\u001b[38;5;241m=\u001b[39mMAX_WORKERS)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# C) DYNAMIC BATCHING\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# We pick some max_tokens, e.g. 2000\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m dyn_time \u001b[38;5;241m=\u001b[39m \u001b[43mrun_dynamic_batching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# 5) Print or estimate time for 1 million tokens\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_estimate\u001b[39m(label: \u001b[38;5;28mstr\u001b[39m, tokens: \u001b[38;5;28mint\u001b[39m, seconds: \u001b[38;5;28mfloat\u001b[39m):\n",
      "Cell \u001b[0;32mIn[38], line 136\u001b[0m, in \u001b[0;36mrun_dynamic_batching\u001b[0;34m(dataset, max_workers, max_tokens)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken_counter_fn\u001b[39m(item: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# We'll add a small overhead (like 10 tokens) for \"prompt framing.\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count_tokens(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 136\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mProcessorConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_batch_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# sub-batch concurrency\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We'll rely on dynamic token chunking\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfail_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_dynamic_token_batching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_counter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_counter_fn\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m client \u001b[38;5;241m=\u001b[39m DeepSeekClient(api_key\u001b[38;5;241m=\u001b[39mAPI_KEY, model\u001b[38;5;241m=\u001b[39mMODEL_NAME, temperature\u001b[38;5;241m=\u001b[39mTEMPERATURE)\n\u001b[1;32m    149\u001b[0m processor \u001b[38;5;241m=\u001b[39m LLMProcessor(llm_client\u001b[38;5;241m=\u001b[39mclient, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "\u001b[0;31mTypeError\u001b[0m: ProcessorConfig.__init__() got an unexpected keyword argument 'enable_dynamic_token_batching'"
     ]
    }
   ],
   "source": [
    "# test_deepseek_speed.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "import tiktoken  # for token counting\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your library\n",
    "from multi_processing.llm_client import DeepSeekClient, BaseLLMClient\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "########################################\n",
    "# 1) Helper: Token Counting with tiktoken\n",
    "########################################\n",
    "# We'll assume a model like 'gpt-3.5-turbo' or 'cl100k_base' encoding.\n",
    "# Adjust if your DeepSeek model uses a different encoding approach.\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "def generate_fake_dataset(num_items: int = 1000, avg_text_length: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random 'text' fields.\n",
    "    Each text is random gibberish ~avg_text_length characters.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=avg_text_length))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) Real DeepSeek Client\n",
    "########################################\n",
    "# We remove the FakeLLMClient; now we do real calls\n",
    "# Make sure you have your DEEPSEEK_API_KEY set or hardcode it below.\n",
    "\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"YOUR_DEEPSEEK_API_KEY\")\n",
    "MODEL_NAME = \"deepseek-chat\"  # Or whichever model you want\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "########################################\n",
    "# 4) SEQUENTIAL: Control Group\n",
    "########################################\n",
    "def run_sequential_no_concurrency(dataset: List[Dict[str, Any]]) -> float:\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    Returns total elapsed time in seconds.\n",
    "    \"\"\"\n",
    "    client = DeepSeekClient(api_key=API_KEY, model=MODEL_NAME, temperature=TEMPERATURE)\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Analyze text: {item['text']}\"\n",
    "        client.call_api(prompt)  # ignoring the content, we only measure speed\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(dataset)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return elapsed\n",
    "\n",
    "########################################\n",
    "# 5) ITEM-LEVEL CONCURRENCY\n",
    "########################################\n",
    "def run_item_concurrency(dataset: List[Dict[str, Any]], max_workers: int) -> float:\n",
    "    \"\"\"\n",
    "    High concurrency (item-level): each item is processed independently by a pool thread.\n",
    "    Returns total elapsed time in seconds.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        enable_batch_prompts=False,\n",
    "        batch_size=1,  # item-level\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        fail_fast=False\n",
    "    )\n",
    "    client = DeepSeekClient(api_key=API_KEY, model=MODEL_NAME, temperature=TEMPERATURE)\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        prompt = f\"Analyze text: {item['text']}\"\n",
    "        return client.call_api(prompt)  # returns e.g. {'content':..., 'success':...}\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        desc=f\"Item-level concurrency ({max_workers} workers)\"\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Item-Level Concurrency] Processed {len(dataset)} items in {elapsed:.2f} seconds.\")\n",
    "    return elapsed\n",
    "\n",
    "########################################\n",
    "# 6) CONCURRENCY + DYNAMIC BATCHING\n",
    "########################################\n",
    "# We'll chunk items by total tokens so each sub-batch is below e.g. 2000 tokens.\n",
    "# Then we process each sub-batch in parallel.\n",
    "########################################\n",
    "\n",
    "def create_batch_prompt(subbatch: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Example: combine multiple items into a single prompt for one LLM call.\n",
    "    \"\"\"\n",
    "    prompt = \"Please analyze the following texts:\\n\\n\"\n",
    "    for itm in subbatch:\n",
    "        prompt += f\"ID={itm['id']} TEXT={itm['text']}\\n\"\n",
    "    prompt += \"\\nProvide a concise analysis.\\n\"\n",
    "    return prompt\n",
    "\n",
    "def run_dynamic_batching(dataset: List[Dict[str, Any]], max_workers: int, max_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    High concurrency with dynamic token-based batching:\n",
    "    Each sub-batch is <= max_tokens in size. \n",
    "    Returns total elapsed time in seconds.\n",
    "    \"\"\"\n",
    "    def token_counter_fn(item: Dict[str, Any]) -> int:\n",
    "        # We'll add a small overhead (like 10 tokens) for \"prompt framing.\"\n",
    "        return count_tokens(item[\"text\"]) + 10\n",
    "    \n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        enable_batch_prompts=True,  # sub-batch concurrency\n",
    "        batch_size=1,  # We'll rely on dynamic token chunking\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        fail_fast=False,\n",
    "        enable_dynamic_token_batching=True,\n",
    "        max_tokens_per_batch=max_tokens,\n",
    "        token_counter_fn=token_counter_fn\n",
    "    )\n",
    "    client = DeepSeekClient(api_key=API_KEY, model=MODEL_NAME, temperature=TEMPERATURE)\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "    \n",
    "    def process_subbatch(subbatch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        For each sub-batch, we build one large prompt.\n",
    "        (In reality, you'd parse the multi-item JSON response,\n",
    "         but here we just measure speed.)\n",
    "        \"\"\"\n",
    "        prompt = create_batch_prompt(subbatch)\n",
    "        return client.call_api(prompt)\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_subbatch,\n",
    "        desc=f\"Dynamic Batching <= {max_tokens} tokens\"\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Dynamic Batching] Processed {len(dataset)} items in {elapsed:.2f} seconds (max_tokens={max_tokens}).\")\n",
    "    return elapsed\n",
    "\n",
    "########################################\n",
    "# 7) UTILITY: Estimate time for 1 million tokens\n",
    "########################################\n",
    "\n",
    "def estimate_time_for_1m_tokens(total_tokens_processed: int, elapsed_seconds: float) -> float:\n",
    "    \"\"\"\n",
    "    Given that we processed `total_tokens_processed` tokens in `elapsed_seconds`,\n",
    "    estimate how long it would take to handle 1,000,000 tokens.\n",
    "    \"\"\"\n",
    "    if total_tokens_processed == 0:\n",
    "        return float('inf')\n",
    "    rate = total_tokens_processed / elapsed_seconds  # tokens per second\n",
    "    time_for_1m = 1_000_000 / rate\n",
    "    return time_for_1m\n",
    "\n",
    "########################################\n",
    "# 8) MAIN: Compare and Calculate\n",
    "########################################\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Adjust these to your desired scale\n",
    "    NUM_ITEMS = 50\n",
    "    AVG_TEXT_LENGTH = 200\n",
    "    MAX_WORKERS = 500\n",
    "\n",
    "    # 2) Generate dataset\n",
    "    dataset = generate_fake_dataset(num_items=NUM_ITEMS, avg_text_length=AVG_TEXT_LENGTH)\n",
    "    \n",
    "    # 3) Count total tokens (approx) in dataset\n",
    "    total_dataset_tokens = 0\n",
    "    for item in dataset:\n",
    "        total_dataset_tokens += count_tokens(item[\"text\"])\n",
    "    print(f\"Generated dataset of {NUM_ITEMS} items, ~{total_dataset_tokens} tokens total.\\n\")\n",
    "\n",
    "    # 4) Run tests\n",
    "    # A) SEQUENTIAL\n",
    "    seq_time = run_sequential_no_concurrency(dataset)\n",
    "    \n",
    "    # B) ITEM-LEVEL CONCURRENCY\n",
    "    item_time = run_item_concurrency(dataset, max_workers=MAX_WORKERS)\n",
    "    \n",
    "    # C) DYNAMIC BATCHING\n",
    "    # We pick some max_tokens, e.g. 2000\n",
    "    dyn_time = run_dynamic_batching(dataset, max_workers=MAX_WORKERS, max_tokens=2000)\n",
    "    \n",
    "    # 5) Print or estimate time for 1 million tokens\n",
    "    def print_estimate(label: str, tokens: int, seconds: float):\n",
    "        million_time = estimate_time_for_1m_tokens(tokens, seconds)\n",
    "        if million_time == float('inf'):\n",
    "            print(f\"{label}: no tokens processed? can't estimate.\\n\")\n",
    "        else:\n",
    "            print(f\"{label}: ~{million_time/60:.2f} minutes for 1M tokens.\\n\")\n",
    "    \n",
    "    print_estimate(\"Sequential\", total_dataset_tokens, seq_time)\n",
    "    print_estimate(\"Item-level concurrency\", total_dataset_tokens, item_time)\n",
    "    print_estimate(\"Dynamic batching\", total_dataset_tokens, dyn_time)\n",
    "\n",
    "    print(\"All done! You can now plot or further analyze these times vs. tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
