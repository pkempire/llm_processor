{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_processing_example.ipynb\n",
    "import pandas as pd\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Setup Configuration\n",
    "config = ProcessorConfig(\n",
    "    batch_size=1,                    # Process 10 videos at a time\n",
    "    max_workers=100,                  # High concurrency for DeepSeek\n",
    "    cache_dir=\"sponsor_cache\",        # Cache directory\n",
    "    save_interval=5,                  # Save every 5 batches\n",
    "    show_progress=True,\n",
    "    metrics_output_path=\"metrics.json\"\n",
    ")\n",
    "\n",
    "# 2. Initialize DeepSeek Client\n",
    "client = DeepSeekClient(\n",
    "    api_key='sk-cd405682db094b6781f9f815840163d8',\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 3. Initialize Processor\n",
    "processor = LLMProcessor(client, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_processing.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VideoData:\n",
    "    \"\"\"Structure for video data\"\"\"\n",
    "    video_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    channel_id: Optional[str] = None\n",
    "    channel_title: Optional[str] = None\n",
    "\n",
    "def create_prompt(videos: List[Dict[str, Any]], desc_length: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Create prompt for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video data dictionaries\n",
    "        desc_length: Max length for description truncation\n",
    "    \"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(videos, 1):\n",
    "        description = video['description']\n",
    "        if len(description) > desc_length:\n",
    "            description = description[:desc_length] + \"...\"\n",
    "            \n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(videos)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content: str) -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Process LLM response into structured sponsor data\n",
    "    \n",
    "    Args:\n",
    "        content: Raw LLM response text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping video IDs to sponsor lists\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean up response if it contains markdown code blocks\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        print(f\"Raw content: {content[:200]}...\")  # Print start of content for debugging\n",
    "        return {}\n",
    "\n",
    "def process_video_batch(batch: Dict[str, Any], client) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single video batch for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing video data\n",
    "        client: LLM client instance\n",
    "    \"\"\"\n",
    "    # Create single-item batch for prompt\n",
    "    batch_list = [batch]\n",
    "    \n",
    "    # Generate and call prompt\n",
    "    prompt = create_prompt(batch_list)\n",
    "    response = client.call_api(prompt)\n",
    "    \n",
    "    # Return structured result\n",
    "    result = {\n",
    "        'video_id': batch['videoId'],\n",
    "        'processed_data': response\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def transform_results(result: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Transform LLM results into structured sponsor records\n",
    "    \n",
    "    Args:\n",
    "        result: Dictionary containing video ID and processed data\n",
    "        \n",
    "    Returns:\n",
    "        List of sponsor records with normalized structure\n",
    "    \"\"\"\n",
    "    video_id = result['video_id']\n",
    "    processed_data = result['processed_data']\n",
    "    \n",
    "    if not processed_data.get('success'):\n",
    "        print(f\"Processing failed for video {video_id}: {processed_data.get('error')}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse sponsors from LLM response\n",
    "    sponsor_data = process_batch_response(processed_data['content'])\n",
    "    sponsors = sponsor_data.get(video_id, [])\n",
    "    \n",
    "    # Create individual records for each sponsor\n",
    "    records = []\n",
    "    for i, sponsor in enumerate(sponsors, 1):\n",
    "        record = {\n",
    "            'video_id': video_id,\n",
    "            f'sponsor_{i}_name': sponsor.get('name'),\n",
    "            f'sponsor_{i}_domain': sponsor.get('domain'),\n",
    "            f'sponsor_{i}_evidence': sponsor.get('evidence')\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "def validate_sponsor_record(record: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate a sponsor record\n",
    "    \n",
    "    Args:\n",
    "        record: Dictionary containing sponsor data\n",
    "        \n",
    "    Returns:\n",
    "        True if record is valid, False otherwise\n",
    "    \"\"\"\n",
    "    required_fields = ['video_id']\n",
    "    sponsor_fields = ['name', 'domain', 'evidence']\n",
    "    \n",
    "    # Check required fields\n",
    "    if not all(field in record for field in required_fields):\n",
    "        return False\n",
    "        \n",
    "    # Check that at least one sponsor exists\n",
    "    has_sponsor = False\n",
    "    i = 1\n",
    "    while f'sponsor_{i}_name' in record:\n",
    "        sponsor_valid = all(\n",
    "            record.get(f'sponsor_{i}_{field}') \n",
    "            for field in sponsor_fields\n",
    "        )\n",
    "        if sponsor_valid:\n",
    "            has_sponsor = True\n",
    "        i += 1\n",
    "        \n",
    "    return has_sponsor\n",
    "\n",
    "def process_sponsor_batch(\n",
    "    videos: List[Dict[str, Any]],\n",
    "    processor,\n",
    "    cache_prefix: str = \"sponsor_detection\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a batch of videos for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video data dictionaries\n",
    "        processor: LLMProcessor instance\n",
    "        cache_prefix: Prefix for cache keys\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing processed sponsor data\n",
    "    \"\"\"\n",
    "    # Process videos through LLM processor\n",
    "    results = processor.process_batch(\n",
    "        items=videos,\n",
    "        process_fn=process_video_batch,\n",
    "        transform_fn=transform_results,\n",
    "        cache_prefix=cache_prefix\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add metadata\n",
    "    df['processed_timestamp'] = pd.Timestamp.now()\n",
    "    df['cache_prefix'] = cache_prefix\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import concurrent.futures\n",
    "from time import perf_counter\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Global caches\n",
    "url_cache = {}\n",
    "domain_cache = {}\n",
    "first_pass_results = {}  # Cache for videos that found sponsors in first pass\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure function execution time\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = perf_counter()\n",
    "        wrapper.total_time += end - start\n",
    "        wrapper.calls += 1\n",
    "        return result\n",
    "    wrapper.total_time = 0\n",
    "    wrapper.calls = 0\n",
    "    return wrapper\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a requests session with retries and timeouts\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "@measure_time\n",
    "def expand_url(url):\n",
    "    \"\"\"Expand shortened URLs with robust caching and handling\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "        \n",
    "    if url in url_cache:\n",
    "        return url_cache[url]\n",
    "        \n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "            \n",
    "        session = create_session()\n",
    "        \n",
    "        # Special handling for known URL shorteners\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        if any(service in domain for service in ['bit.ly', 'goo.gl', 'tinyurl']):\n",
    "            response = session.get(\n",
    "                url,\n",
    "                allow_redirects=True,\n",
    "                timeout=10,\n",
    "                headers={'User-Agent': 'Mozilla/5.0'},\n",
    "                stream=True\n",
    "            )\n",
    "        else:\n",
    "            response = session.head(\n",
    "                url,\n",
    "                allow_redirects=True,\n",
    "                timeout=5\n",
    "            )\n",
    "        \n",
    "        final_url = response.url\n",
    "        if isinstance(response, requests.models.Response):\n",
    "            response.close()\n",
    "        \n",
    "        url_cache[url] = final_url\n",
    "        return final_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"URL expansion error for {url}: {e}\")\n",
    "        return url\n",
    "\n",
    "def quick_domain_extract(url):\n",
    "    \"\"\"Extract domain with improved caching\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    if url_lower in domain_cache:\n",
    "        return domain_cache[url_lower]\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url_lower)\n",
    "        domain = parsed.netloc or parsed.path.split('/')[0]\n",
    "        \n",
    "        for prefix in ['www.']:\n",
    "            if domain.startswith(prefix):\n",
    "                domain = domain[len(prefix):]\n",
    "                \n",
    "        domain_cache[url_lower] = domain\n",
    "        return domain\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"Create unified prompt for both passes\"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, row in enumerate(video_batch.iterrows(), 1):\n",
    "        _, video = row\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content):\n",
    "    \"\"\"Process LLM response without URL expansion\"\"\"\n",
    "    try:\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_batch(batch, is_second_pass=False):\n",
    "    \"\"\"Process one batch with two-pass system\"\"\"\n",
    "    try:\n",
    "        # Skip videos that already have sponsors from first pass\n",
    "        if is_second_pass:\n",
    "            batch = batch[~batch['videoId'].isin(first_pass_results.keys())]\n",
    "            if batch.empty:\n",
    "                return {}\n",
    "        \n",
    "        # Use appropriate description length\n",
    "        desc_length = 1500 if is_second_pass else 200\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": create_prompt(batch, desc_length)\n",
    "            }],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        batch_results = process_batch_response(content)\n",
    "        \n",
    "        # Cache first pass results\n",
    "        if not is_second_pass:\n",
    "            first_pass_results.update(batch_results)\n",
    "        \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {'second' if is_second_pass else 'first'} pass: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_videos_parallel(df, batch_size=5, max_workers=3):\n",
    "    \"\"\"Process videos with two-pass system\"\"\"\n",
    "    sponsor_map = {}\n",
    "    batches = [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Processing videos (first pass)\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # First pass - 200 tokens\n",
    "            futures = [executor.submit(process_batch, batch, False) for batch in batches]\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    sponsor_map.update(results)\n",
    "                    pbar.update(batch_size)\n",
    "                except Exception as e:\n",
    "                    print(f\"Batch processing failed: {e}\")\n",
    "    \n",
    "    # Second pass for videos without sponsors\n",
    "    remaining_videos = len(df) - len(first_pass_results)\n",
    "    if remaining_videos > 0:\n",
    "        with tqdm(total=remaining_videos, desc=\"Processing videos (second pass)\") as pbar:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                futures = [executor.submit(process_batch, batch, True) for batch in batches]\n",
    "                \n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    try:\n",
    "                        results = future.result()\n",
    "                        sponsor_map.update(results)\n",
    "                        pbar.update(len(results))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Batch processing failed: {e}\")\n",
    "    \n",
    "    return sponsor_map\n",
    "\n",
    "def expand_sponsor_data(df, sponsor_map):\n",
    "    \"\"\"Expand sponsor data into columns efficiently\"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find max sponsors accounting for all videos\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for all possible sponsors\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def save_checkpoint(df, sponsor_map, current_idx, pass_number):\n",
    "    \"\"\"Save processing checkpoint with pass information\"\"\"\n",
    "    temp_df = df.copy()\n",
    "    temp_df['sponsor_data'] = temp_df['videoId'].map(lambda x: json.dumps(sponsor_map.get(x, [])))\n",
    "    temp_df.to_csv(f'sponsors_checkpoint_pass{pass_number}_{current_idx}.csv', index=False)\n",
    "\n",
    "def print_stats(df, sponsor_map):\n",
    "    \"\"\"Print detailed processing statistics\"\"\"\n",
    "    first_pass_count = len(first_pass_results)\n",
    "    second_pass_count = len(sponsor_map) - first_pass_count\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    print(f\"Videos with sponsors found in first pass (200 tokens): {first_pass_count}\")\n",
    "    print(f\"Additional sponsors found in second pass (1500 tokens): {second_pass_count}\")\n",
    "    print(f\"Total videos with sponsors: {len(sponsor_map)}\")\n",
    "    \n",
    "    # URL expansion stats\n",
    "    if expand_url.calls > 0:\n",
    "        avg_time = expand_url.total_time / expand_url.calls\n",
    "        print(f\"\\nURL Processing:\")\n",
    "        print(f\"Total URLs processed: {expand_url.calls}\")\n",
    "        print(f\"Average processing time: {avg_time:.2f}s per URL\")\n",
    "        print(f\"Cache hits: {len(url_cache)}\")\n",
    "    \n",
    "    # Sponsor distribution\n",
    "    sponsor_counts = [len(sponsors) for sponsors in sponsor_map.values()]\n",
    "    if sponsor_counts:\n",
    "        print(\"\\nSponsor Distribution:\")\n",
    "        print(f\"Average sponsors per video: {sum(sponsor_counts)/len(sponsor_counts):.2f}\")\n",
    "        print(f\"Max sponsors in a video: {max(sponsor_counts)}\")\n",
    "        \n",
    "        count_distribution = pd.Series(sponsor_counts).value_counts().sort_index()\n",
    "        print(\"\\nVideos by sponsor count:\")\n",
    "        for count, videos in count_distribution.items():\n",
    "            print(f\"{count} sponsor(s): {videos} videos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read sample data\n",
    "        df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "        \n",
    "        # Process videos with two-pass system\n",
    "        sponsor_map = process_videos_parallel(\n",
    "            df, \n",
    "            batch_size=10,  # Adjust based on your testing\n",
    "            max_workers=100   # Adjust based on your CPU\n",
    "        )\n",
    "        \n",
    "        # Expand and save sponsor data\n",
    "        final_df = expand_sponsor_data(df, sponsor_map)\n",
    "        final_df.to_csv('sponsors_chunk_8.csv', index=False)\n",
    "        \n",
    "        # Print detailed stats\n",
    "        print_stats(df, sponsor_map)\n",
    "        \n",
    "        # Print sample of found sponsors\n",
    "        print(\"\\nSample Sponsors Found:\")\n",
    "        sample_videos = list(sponsor_map.items())[:3]\n",
    "        for video_id, sponsors in sample_videos:\n",
    "            print(f\"\\nVideo {video_id}:\")\n",
    "            for i, sponsor in enumerate(sponsors, 1):\n",
    "                print(f\"  Sponsor {i}:\")\n",
    "                print(f\"    Name: {sponsor.get('name')}\")\n",
    "                print(f\"    Domain: {sponsor.get('domain')}\")\n",
    "                print(f\"    Evidence: {sponsor.get('evidence')[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/16678 [00:16<25:36:23,  5.53s/it, processed=3, success_rate=0.0%, errors=0, avg_time=4.29s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 288\u001b[0m\n\u001b[1;32m    285\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Process with library\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_videos_with_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msk-cd405682db094b6781f9f815840163d8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    294\u001b[0m final_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[29], line 178\u001b[0m, in \u001b[0;36mprocess_videos_with_library\u001b[0;34m(df, api_key)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Process first pass\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m first_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_first_pass\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Process second pass only for remaining videos\u001b[39;00m\n\u001b[1;32m    184\u001b[0m remaining \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    185\u001b[0m     v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m first_pass_results\n\u001b[1;32m    187\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/multi_llm/llm_processor/multi_processing/processor.py:185\u001b[0m, in \u001b[0;36mLLMProcessor.process_batch\u001b[0;34m(self, items, process_fn, transform_fn, cache_prefix, use_cache, output_path)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    179\u001b[0m     fn \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_with_retry,\n\u001b[1;32m    181\u001b[0m         process_fn\u001b[38;5;241m=\u001b[39mprocess_fn,\n\u001b[1;32m    182\u001b[0m         cache_prefix\u001b[38;5;241m=\u001b[39mcache_prefix,\n\u001b[1;32m    183\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache\n\u001b[1;32m    184\u001b[0m     )\n\u001b[0;32m--> 185\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(fn, batch_data))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Transform results if needed\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_fn:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sponsor_processor.py\n",
    "\n",
    "from multi_processing.processor import LLMProcessor, ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"Create unified prompt for both passes\"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(video_batch, 1):\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content):\n",
    "    \"\"\"Process LLM response without URL expansion\"\"\"\n",
    "    try:\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return {}\n",
    "\n",
    "def expand_sponsor_data(df, sponsor_map):\n",
    "    \"\"\"Expand sponsor data into columns efficiently\"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find max sponsors accounting for all videos\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for all possible sponsors\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def print_stats(df, sponsor_map):\n",
    "    \"\"\"Print detailed processing statistics\"\"\"\n",
    "    first_pass_count = len(first_pass_results)\n",
    "    second_pass_count = len(sponsor_map) - first_pass_count\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    print(f\"Videos with sponsors found in first pass (200 tokens): {first_pass_count}\")\n",
    "    print(f\"Additional sponsors found in second pass (1500 tokens): {second_pass_count}\")\n",
    "    print(f\"Total videos with sponsors: {len(sponsor_map)}\")\n",
    "    \n",
    "    # Sponsor distribution\n",
    "    sponsor_counts = [len(sponsors) for sponsors in sponsor_map.values()]\n",
    "    if sponsor_counts:\n",
    "        print(\"\\nSponsor Distribution:\")\n",
    "        print(f\"Average sponsors per video: {sum(sponsor_counts)/len(sponsor_counts):.2f}\")\n",
    "        print(f\"Max sponsors in a video: {max(sponsor_counts)}\")\n",
    "        \n",
    "        count_distribution = pd.Series(sponsor_counts).value_counts().sort_index()\n",
    "        print(\"\\nVideos by sponsor count:\")\n",
    "        for count, videos in count_distribution.items():\n",
    "            print(f\"{count} sponsor(s): {videos} videos\")\n",
    "\n",
    "# New wrapper for library integration\n",
    "def process_videos_with_library(df: pd.DataFrame, api_key: str):\n",
    "    \"\"\"Process videos using library's built-in parallel processing\"\"\"\n",
    "    \n",
    "    # Initialize basic config\n",
    "    config = ProcessorConfig(\n",
    "    cache_enabled=False,     # No disk cache\n",
    "    rate_limit=0.0,          # No forced sleep between calls\n",
    "    max_retries=1,           # Or 2, if you rarely fail\n",
    "    batch_size=1,            # Not crucial if you're doing item-level concurrency anyway\n",
    "    max_workers=100,         # If your system can handle it\n",
    "    fail_fast=True,\n",
    "    # ... etc.\n",
    ")\n",
    "    \n",
    "    client = DeepSeekClient(api_key=api_key)\n",
    "    processor = LLMProcessor(client, config)\n",
    "    \n",
    "    # Track first pass results\n",
    "    first_pass_results = {}\n",
    "    \n",
    "    # Define processing functions\n",
    "    def process_first_pass(video):\n",
    "        \"\"\"First pass with short descriptions\"\"\"\n",
    "        response = client.call_api(\n",
    "            create_prompt([video], desc_length=200)\n",
    "        )\n",
    "        if response['success']:\n",
    "            results = process_batch_response(response['content'])\n",
    "            first_pass_results.update(results)\n",
    "        return results\n",
    "\n",
    "    # Process first pass\n",
    "    first_results = processor.process_batch(\n",
    "        items=df.to_dict('records'),\n",
    "        process_fn=process_first_pass\n",
    "    )\n",
    "    \n",
    "    # Process second pass only for remaining videos\n",
    "    remaining = [\n",
    "        v for v in df.to_dict('records')\n",
    "        if v['videoId'] not in first_pass_results\n",
    "    ]\n",
    "    \n",
    "    def process_second_pass(video):\n",
    "        \"\"\"Second pass with longer descriptions\"\"\"\n",
    "        response = client.call_api(\n",
    "            create_prompt([video], desc_length=1500)\n",
    "        )\n",
    "        if response['success']:\n",
    "            return process_batch_response(response['content'])\n",
    "        return {}\n",
    "\n",
    "    second_results = processor.process_batch(\n",
    "        items=remaining,\n",
    "        process_fn=process_second_pass\n",
    "    )\n",
    "    \n",
    "    # Combine results and expand\n",
    "    all_results = {**first_results, **second_results}\n",
    "    return expand_sponsor_data(df, all_results)\n",
    "\n",
    "    \n",
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "\n",
    "class Telemetry:\n",
    "    def __init__(self):\n",
    "        self.timings = defaultdict(list)\n",
    "        self.counts = defaultdict(int)\n",
    "        \n",
    "    def measure(self, operation):\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start = perf_counter()\n",
    "                result = func(*args, **kwargs)\n",
    "                duration = perf_counter() - start\n",
    "                self.timings[operation].append(duration)\n",
    "                self.counts[operation] += 1\n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def print_stats(self):\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        for op, times in self.timings.items():\n",
    "            avg_time = sum(times) / len(times)\n",
    "            total_time = sum(times)\n",
    "            print(f\"\\n{op}:\")\n",
    "            print(f\"  Count: {self.counts[op]}\")\n",
    "            print(f\"  Average time: {avg_time:.2f}s\")\n",
    "            print(f\"  Total time: {total_time:.2f}s\")\n",
    "            print(f\"  % of total time: {(total_time/sum(sum(t) for t in self.timings.values()))*100:.1f}%\")\n",
    "\n",
    "# Add to your processor\n",
    "telemetry = Telemetry()\n",
    "\n",
    "@telemetry.measure(\"API Call\")\n",
    "def process_batch(batch, is_second_pass=False):\n",
    "    \"\"\"Process batch with timing\"\"\"\n",
    "    try:\n",
    "        if is_second_pass:\n",
    "            batch = [v for v in batch if v['videoId'] not in first_pass_results]\n",
    "            if not batch:\n",
    "                return {}\n",
    "        \n",
    "        desc_length = 1500 if is_second_pass else 200\n",
    "        \n",
    "        # Measure prompt creation\n",
    "        start = perf_counter()\n",
    "        prompt = create_prompt(batch, desc_length)\n",
    "        telemetry.timings[\"prompt_creation\"].append(perf_counter() - start)\n",
    "        \n",
    "        # Measure API call\n",
    "        start = perf_counter()\n",
    "        response = client.call_api(prompt)\n",
    "        telemetry.timings[\"pure_api_call\"].append(perf_counter() - start)\n",
    "        \n",
    "        if not response['success']:\n",
    "            return {}\n",
    "            \n",
    "        # Measure response processing    \n",
    "        start = perf_counter()\n",
    "        batch_results = process_batch_response(response['content'])\n",
    "        telemetry.timings[\"response_processing\"].append(perf_counter() - start)\n",
    "        \n",
    "        if not is_second_pass:\n",
    "            first_pass_results.update(batch_results)\n",
    "            \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {'second' if is_second_pass else 'first'} pass: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read data\n",
    "        df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "        \n",
    "        # Process with library\n",
    "        final_df = process_videos_with_library(\n",
    "            df,\n",
    "            api_key='sk-cd405682db094b6781f9f815840163d8'\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        final_df.to_csv('sponsor_results.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 16678/16678 [00:00<00:00, 21456.73it/s]\n",
      "Processing items: 100%|██████████| 16678/16678 [00:00<00:00, 26364.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Results saved to sponsor_results.csv\n"
     ]
    }
   ],
   "source": [
    "# sponsor_processor.py\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#########################################\n",
    "# Helper Functions\n",
    "#########################################\n",
    "\n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"\n",
    "    Create a unified prompt for sponsor extraction.\n",
    "    Each item in video_batch is a dict with keys: videoId, title, description, etc.\n",
    "    \"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(video_batch, 1):\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Parse the JSON returned by the LLM and extract a dict:\n",
    "    { video_id: [ {name, domain, evidence}, ... ] }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If the LLM wraps JSON in code fences, strip them out\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "def expand_sponsor_data(df: pd.DataFrame, sponsor_map: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add columns for each sponsor (name, domain, evidence) to the original DataFrame.\n",
    "    \"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find the maximum number of sponsors that any video has\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        # For each sponsor slot, create columns\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for the sponsor slots\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with the original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    return final_df\n",
    "\n",
    "#########################################\n",
    "# Two-Pass Sponsor Processing\n",
    "#########################################\n",
    "\n",
    "def process_videos_with_library(df: pd.DataFrame, api_key: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Two-pass sponsor extraction using the LLMProcessor for concurrency.\n",
    "    1) First pass with short (200 tokens) description\n",
    "    2) Second pass with longer (1500 tokens) description\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Create an LLMProcessor with item-level concurrency & no disk cache\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,  # Turn off caching to avoid disk overhead\n",
    "        enable_batch_prompts=True,\n",
    "        rate_limit=0.0,       # No forced sleep\n",
    "        max_retries=1,        # Minimal retry\n",
    "        batch_size=1,         # Let each item be processed individually\n",
    "        max_workers=100,       # Adjust to your hardware (could be 50, 100, etc.)\n",
    "        fail_fast=False\n",
    "    )\n",
    "    client = DeepSeekClient(api_key=api_key, model=\"deepseek-chat\")\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    # 2. A dictionary to store all results across both passes: { videoId -> list_of_sponsors }\n",
    "    all_sponsors = {}\n",
    "\n",
    "    # 3. Prepare the \"process function\" for the library (1st pass)\n",
    "    def process_first_pass(video: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handles a single item (video) with short prompt (200 tokens).\"\"\"\n",
    "        prompt = create_prompt([video], desc_length=200)\n",
    "        response = client.call_api(prompt)\n",
    "        if not response.get('success'):\n",
    "            return {\"videoId\": video[\"videoId\"], \"sponsors\": []}\n",
    "\n",
    "        parsed = process_batch_response(response['content'])\n",
    "        sponsors_for_vid = parsed.get(video[\"videoId\"], [])\n",
    "        # Return a dict so the processor can gather results\n",
    "        return {\"videoId\": video[\"videoId\"], \"sponsors\": sponsors_for_vid}\n",
    "\n",
    "    # 4. First pass: pass all videos\n",
    "    first_pass_items = df.to_dict(\"records\")  # list of dicts\n",
    "    first_pass_results = processor.process_batch(\n",
    "        items=first_pass_items,\n",
    "        process_fn=process_first_pass,\n",
    "        use_cache=False  # explicitly disable caching\n",
    "    )\n",
    "\n",
    "    # 5. Store first pass results\n",
    "    for res in first_pass_results:\n",
    "        all_sponsors[res[\"videoId\"]] = res[\"sponsors\"]\n",
    "\n",
    "    # 6. Second pass: only for videos that had no sponsors in first pass\n",
    "    def process_second_pass(video: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Handles a single item (video) with longer prompt (1500 tokens).\"\"\"\n",
    "        prompt = create_prompt([video], desc_length=1500)\n",
    "        response = client.call_api(prompt)\n",
    "        if not response.get('success'):\n",
    "            return {\"videoId\": video[\"videoId\"], \"sponsors\": []}\n",
    "        \n",
    "        parsed = process_batch_response(response['content'])\n",
    "        sponsors_for_vid = parsed.get(video[\"videoId\"], [])\n",
    "        return {\"videoId\": video[\"videoId\"], \"sponsors\": sponsors_for_vid}\n",
    "\n",
    "    remaining_items = [\n",
    "        vid for vid in df.to_dict(\"records\")\n",
    "        if not all_sponsors.get(vid[\"videoId\"])  # only if empty/None\n",
    "    ]\n",
    "    if remaining_items:\n",
    "        second_pass_results = processor.process_batch(\n",
    "            items=remaining_items,\n",
    "            process_fn=process_second_pass,\n",
    "            use_cache=False\n",
    "        )\n",
    "        for res in second_pass_results:\n",
    "            all_sponsors[res[\"videoId\"]] = res[\"sponsors\"]\n",
    "\n",
    "    # 7. Expand into final DataFrame columns\n",
    "    final_df = expand_sponsor_data(df, all_sponsors)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Example Main\n",
    "#########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load your CSV\n",
    "        df = pd.read_csv(\"/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv\")\n",
    "        \n",
    "        # Put your actual DeepSeek/LLM API key here\n",
    "        API_KEY = \"sk-cd405682db094b6781f9f815840163d8\"\n",
    "\n",
    "        # Run the two-pass sponsor extraction\n",
    "        final_df = process_videos_with_library(df, API_KEY)\n",
    "\n",
    "        # Save results\n",
    "        final_df.to_csv(\"sponsor_results.csv\", index=False)\n",
    "        \n",
    "        print(\"Processing complete! Results saved to sponsor_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running sponsor processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_video_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m video_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process videos with our framework\u001b[39;00m\n\u001b[1;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mprocess_batch(\n\u001b[1;32m     10\u001b[0m     items\u001b[38;5;241m=\u001b[39mvideo_data,\n\u001b[0;32m---> 11\u001b[0m     process_fn\u001b[38;5;241m=\u001b[39m\u001b[43mprocess_video_batch\u001b[49m,\n\u001b[1;32m     12\u001b[0m     transform_fn\u001b[38;5;241m=\u001b[39mtransform_results,\n\u001b[1;32m     13\u001b[0m     cache_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_extraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 6. Analyze Results\u001b[39;00m\n\u001b[1;32m     18\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_video_batch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Load and Process Data\n",
    "# Load your video data\n",
    "df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "\n",
    "# Convert DataFrame rows to list of dicts\n",
    "video_data = df.to_dict('records')\n",
    "\n",
    "# Process videos with our framework\n",
    "results = processor.process_batch(\n",
    "    items=video_data,\n",
    "    process_fn=process_video_batch,\n",
    "    transform_fn=transform_results,\n",
    "    cache_prefix='sponsor_extraction',\n",
    "    output_path='sponsor_results.csv'\n",
    ")\n",
    "\n",
    "# 6. Analyze Results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nProcessing Statistics:\")\n",
    "print(f\"Total videos processed: {len(df)}\")\n",
    "print(f\"Videos with sponsors found: {len(results_df)}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample Sponsor Results:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# 7. Check Processing Metrics\n",
    "import json\n",
    "with open(\"metrics.json\", 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"\\nProcessing Metrics:\")\n",
    "print(f\"Total processing time: {metrics['total_time']:.2f} seconds\")\n",
    "print(f\"Average time per item: {metrics['avg_process_time']:.2f} seconds\")\n",
    "print(f\"Cache hits: {metrics['cache_hits']}\")\n",
    "print(f\"Total errors: {metrics['errors']}\")\n",
    "\n",
    "# 8. Additional Analysis\n",
    "if results_df.empty:\n",
    "    print(\"No results found\")\n",
    "else:\n",
    "    # Get sponsor frequency\n",
    "    sponsor_cols = [col for col in results_df.columns if 'sponsor_' in col and 'name' in col]\n",
    "    all_sponsors = results_df[sponsor_cols].values.flatten()\n",
    "    sponsor_counts = pd.Series(all_sponsors).value_counts().dropna()\n",
    "\n",
    "    print(\"\\nTop Sponsors:\")\n",
    "    print(sponsor_counts.head())\n",
    "\n",
    "# 9. Save Final Results\n",
    "results_df.to_csv('final_sponsor_analysis.csv', index=False)\n",
    "print(\"\\nResults saved to 'final_sponsor_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 200/200 [00:04<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library] Processed 200 items in 4.24 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw concurrency: 100%|██████████| 200/200 [00:04<00:00, 47.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Concurrency] Processed 200 items in 4.21 seconds.\n",
      "Library results: 200 items.\n",
      "Raw concurrency results: 200 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_processor_speed.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# 1) Fake LLMClient to Simulate API Calls\n",
    "########################################\n",
    "\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class FakeLLMClient(BaseLLMClient):\n",
    "    \"\"\"\n",
    "    A fake client that simulates a 100-300ms \"API call\" time.\n",
    "    No real network usage, just time.sleep().\n",
    "    \"\"\"\n",
    "\n",
    "    def call_api(self, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        # Sleep a random time to simulate latency\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        # Return a pretend success payload\n",
    "        return {\n",
    "            \"content\": f\"Fake response for prompt: {prompt[:30]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def validate_response(self, response: Dict[str, Any]) -> bool:\n",
    "        return response.get(\"success\", False)\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "\n",
    "def generate_fake_dataset(num_items: int = 2000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random text data.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=100))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) The \"process function\" we apply\n",
    "########################################\n",
    "\n",
    "def process_item_with_fake_llm(item: Dict[str, Any], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a single item, call the fake LLM API and return a small result dict.\n",
    "    \"\"\"\n",
    "    prompt = f\"Process text: {item['text']}\"\n",
    "    response = client.call_api(prompt)\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"content\": response[\"content\"],  # from the fake LLM\n",
    "        \"success\": response[\"success\"]\n",
    "    }\n",
    "\n",
    "def run_sequential_no_concurrency(dataset, client):\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Process text: {item['text']}\"\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"content\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        })\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(results)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "########################################\n",
    "# 4) Testing with the LLMProcessor Library\n",
    "########################################\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "def run_with_library(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use your LLMProcessor with concurrency, no caching, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,  # no disk caching\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=1,         # item-level concurrency\n",
    "        fail_fast=False\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()  # Our fake client\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        return process_item_with_fake_llm(item, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    results = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\",   # not used if cache is disabled\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Library] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 5) Testing with Raw Concurrency (ThreadPool)\n",
    "########################################\n",
    "\n",
    "def run_with_raw_concurrency(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use a plain ThreadPoolExecutor approach, no library overhead.\n",
    "    \"\"\"\n",
    "    client = FakeLLMClient()\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=\"Raw concurrency\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in dataset:\n",
    "                futures.append(executor.submit(process_item_with_fake_llm, item, client))\n",
    "            \n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                res = f.result()\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Raw Concurrency] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 6) Main Comparison\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate a sample dataset\n",
    "    data_size = 200  # adjust to see bigger difference\n",
    "    dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "    # 2) Run with library\n",
    "    library_results = run_with_library(dataset, max_workers=10)\n",
    "\n",
    "    # 3) Run with raw concurrency\n",
    "    raw_results = run_with_raw_concurrency(dataset, max_workers=10)\n",
    "\n",
    "    control_results = run_sequential_no_concurrency(dataset, client)\n",
    "\n",
    "    # 4) Quick check\n",
    "    # Validate that we got the same number of results\n",
    "    print(f\"Library results: {len(library_results)} items.\")\n",
    "    print(f\"Raw concurrency results: {len(raw_results)} items.\")\n",
    "    \n",
    "    # If you want to confirm the outputs are consistent, you can compare them\n",
    "    # but here we only compare time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential (control): 100%|██████████| 200/200 [00:39<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Control] Processed 200 items sequentially in 39.41 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 200/200 [00:03<00:00, 50.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library Item-Level] Processed 200 items in 3.96 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Library Batch Mode: 100%|██████████| 20/20 [00:00<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library Batch Mode] Processed 200 items in 0.44 seconds (sub-batch size=10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw concurrency: 100%|██████████| 200/200 [00:04<00:00, 45.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Concurrency] Processed 200 items in 4.44 seconds.\n",
      "\n",
      "Control results: 200 items.\n",
      "Library item-level results: 200 items.\n",
      "Library batch-mode results: 200 items.\n",
      "Raw concurrency results: 200 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_processor_speed.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# 1) Fake LLMClient to Simulate API Calls\n",
    "########################################\n",
    "\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class FakeLLMClient(BaseLLMClient):\n",
    "    \"\"\"\n",
    "    A fake client that simulates a 100-300ms \"API call\" time.\n",
    "    No real network usage, just time.sleep().\n",
    "    \"\"\"\n",
    "\n",
    "    def call_api(self, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        # Sleep a random time to simulate latency\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        # Return a pretend success payload\n",
    "        return {\n",
    "            \"content\": f\"Fake response for prompt: {prompt[:30]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def validate_response(self, response: Dict[str, Any]) -> bool:\n",
    "        return response.get(\"success\", False)\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "\n",
    "def generate_fake_dataset(num_items: int = 2000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random text data.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=100))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) The \"process function\" for item-level\n",
    "########################################\n",
    "\n",
    "def process_item_with_fake_llm(item: Dict[str, Any], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a single item, call the fake LLM API and return a small result dict.\n",
    "    \"\"\"\n",
    "    prompt = f\"Process text: {item['text']}\"\n",
    "    response = client.call_api(prompt)\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"content\": response[\"content\"],  # from the fake LLM\n",
    "        \"success\": response[\"success\"]\n",
    "    }\n",
    "\n",
    "########################################\n",
    "# 4) No concurrency: control group\n",
    "########################################\n",
    "\n",
    "def run_sequential_no_concurrency(dataset, client):\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Process text: {item['text']}\"\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"content\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        })\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(results)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 5) The Library with item-level concurrency\n",
    "########################################\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "def run_with_library(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use your LLMProcessor with concurrency, no caching, etc., item-level.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=1,         # item-level concurrency\n",
    "        fail_fast=False,\n",
    "        enable_batch_prompts=False  # <== ensure item-level\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        return process_item_with_fake_llm(item, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    results = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\", \n",
    "        use_cache=False\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Library Item-Level] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 6) The Library with \"Batch\" prompts\n",
    "########################################\n",
    "\n",
    "def create_subbatch_prompt(items: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Example: combine multiple items into a single prompt.\n",
    "    items might be up to config.batch_size in length.\n",
    "    \"\"\"\n",
    "    # Just build a single text listing them\n",
    "    combined_text = \"\"\n",
    "    for itm in items:\n",
    "        combined_text += f\"(ID={itm['id']}) {itm['text']}\\n\"\n",
    "    prompt = f\"Process these {len(items)} texts at once:\\n{combined_text}\"\n",
    "    return prompt\n",
    "\n",
    "def process_subbatch(subbatch: List[Dict[str, Any]], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Called once per sub-batch. We call the LLM once for all items in subbatch.\n",
    "    Return a dict with the results. For example, { item_id -> info }.\n",
    "    \"\"\"\n",
    "    prompt = create_subbatch_prompt(subbatch)\n",
    "    response = client.call_api(prompt)\n",
    "    \n",
    "    # We'll pretend we parse out something. For now, just store the prompt.\n",
    "    # In a real scenario, you'd parse JSON containing all items' results.\n",
    "    result_map = {}\n",
    "    for itm in subbatch:\n",
    "        result_map[itm['id']] = {\n",
    "            \"id\": itm[\"id\"],\n",
    "            \"text\": itm[\"text\"],\n",
    "            \"combined_response\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        }\n",
    "    return result_map\n",
    "\n",
    "def run_with_library_batch_mode(dataset: List[Dict[str, Any]], max_workers: int = 10, batch_size: int = 10):\n",
    "    \"\"\"\n",
    "    Concurrency across sub-batches (enable_batch_prompts=True).\n",
    "    Each sub-batch calls the LLM once for multiple items.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=batch_size,  # sub-batch size\n",
    "        fail_fast=False,\n",
    "        enable_batch_prompts=True  # <== batch mode\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(subbatch: List[Dict[str, Any]]) -> Dict[int, Any]:\n",
    "        return process_subbatch(subbatch, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    # This returns a list of dicts. Each dict is { item_id -> info } for one sub-batch\n",
    "    dict_list = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\", \n",
    "        use_cache=False,\n",
    "        desc=\"Library Batch Mode\"\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "\n",
    "    # Combine all subdicts\n",
    "    combined = {}\n",
    "    for subdict in dict_list:\n",
    "        combined.update(subdict)  # merges item_id -> info\n",
    "    \n",
    "    print(f\"[Library Batch Mode] Processed {len(combined)} items in {elapsed:.2f} seconds (sub-batch size={batch_size}).\")\n",
    "    return combined\n",
    "\n",
    "########################################\n",
    "# 7) Raw Concurrency (ThreadPool)\n",
    "########################################\n",
    "\n",
    "def run_with_raw_concurrency(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use a plain ThreadPoolExecutor approach, no library overhead.\n",
    "    \"\"\"\n",
    "    client = FakeLLMClient()\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=\"Raw concurrency\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in dataset:\n",
    "                futures.append(executor.submit(process_item_with_fake_llm, item, client))\n",
    "            \n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                res = f.result()\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Raw Concurrency] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 8) Main Comparison\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate a sample dataset\n",
    "    data_size = 500\n",
    "    dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "    # We'll re-use one FakeLLMClient for the control\n",
    "    control_client = FakeLLMClient()\n",
    "    \n",
    "    # 2) Control: No concurrency\n",
    "    control_results = run_sequential_no_concurrency(dataset, control_client)\n",
    "\n",
    "    # 3) Library, item-level concurrency\n",
    "    library_item_results = run_with_library(dataset, max_workers=100)\n",
    "\n",
    "    # 4) Library, batch mode concurrency\n",
    "    #    Each sub-batch processes multiple items in a single fake API call\n",
    "    library_batch_results = run_with_library_batch_mode(dataset, max_workers=100, batch_size=10)\n",
    "\n",
    "    # 5) Raw concurrency\n",
    "    raw_results = run_with_raw_concurrency(dataset, max_workers=100)\n",
    "\n",
    "    # 6) Print final comparisons\n",
    "    print(f\"\\nControl results: {len(control_results)} items.\")\n",
    "    print(f\"Library item-level results: {len(library_item_results)} items.\")\n",
    "    print(f\"Library batch-mode results: {len(library_batch_results)} items.\")\n",
    "    print(f\"Raw concurrency results: {len(raw_results)} items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mget(item))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcount_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m, in \u001b[0;36mcount_tokens\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(item))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "data_size = 500\n",
    "dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "def count_tokens(text: dict) -> int:\n",
    "    sum =0\n",
    "    for item in text:\n",
    "        sum += len(dataset.get(item))\n",
    "    return sum\n",
    "count_tokens(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "dataset.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
