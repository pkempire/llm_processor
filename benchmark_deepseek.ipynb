{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_processing_example.ipynb\n",
    "import pandas as pd\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Setup Configuration\n",
    "config = ProcessorConfig(\n",
    "    batch_size=1,                    # Process 10 videos at a time\n",
    "    max_workers=100,                  # High concurrency for DeepSeek\n",
    "    cache_dir=\"sponsor_cache\",        # Cache directory\n",
    "    save_interval=5,                  # Save every 5 batches\n",
    "    show_progress=True,\n",
    "    metrics_output_path=\"metrics.json\"\n",
    ")\n",
    "\n",
    "# 2. Initialize DeepSeek Client\n",
    "client = DeepSeekClient(\n",
    "    api_key='sk-cd405682db094b6781f9f815840163d8',\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 3. Initialize Processor\n",
    "processor = LLMProcessor(client, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_processing.py\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VideoData:\n",
    "    \"\"\"Structure for video data\"\"\"\n",
    "    video_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    channel_id: Optional[str] = None\n",
    "    channel_title: Optional[str] = None\n",
    "\n",
    "def create_prompt(videos: List[Dict[str, Any]], desc_length: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Create prompt for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video data dictionaries\n",
    "        desc_length: Max length for description truncation\n",
    "    \"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(videos, 1):\n",
    "        description = video['description']\n",
    "        if len(description) > desc_length:\n",
    "            description = description[:desc_length] + \"...\"\n",
    "            \n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(videos)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content: str) -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Process LLM response into structured sponsor data\n",
    "    \n",
    "    Args:\n",
    "        content: Raw LLM response text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping video IDs to sponsor lists\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean up response if it contains markdown code blocks\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        print(f\"Raw content: {content[:200]}...\")  # Print start of content for debugging\n",
    "        return {}\n",
    "\n",
    "def process_video_batch(batch: Dict[str, Any], client) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single video batch for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing video data\n",
    "        client: LLM client instance\n",
    "    \"\"\"\n",
    "    # Create single-item batch for prompt\n",
    "    batch_list = [batch]\n",
    "    \n",
    "    # Generate and call prompt\n",
    "    prompt = create_prompt(batch_list)\n",
    "    response = client.call_api(prompt)\n",
    "    \n",
    "    # Return structured result\n",
    "    result = {\n",
    "        'video_id': batch['videoId'],\n",
    "        'processed_data': response\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def transform_results(result: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Transform LLM results into structured sponsor records\n",
    "    \n",
    "    Args:\n",
    "        result: Dictionary containing video ID and processed data\n",
    "        \n",
    "    Returns:\n",
    "        List of sponsor records with normalized structure\n",
    "    \"\"\"\n",
    "    video_id = result['video_id']\n",
    "    processed_data = result['processed_data']\n",
    "    \n",
    "    if not processed_data.get('success'):\n",
    "        print(f\"Processing failed for video {video_id}: {processed_data.get('error')}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse sponsors from LLM response\n",
    "    sponsor_data = process_batch_response(processed_data['content'])\n",
    "    sponsors = sponsor_data.get(video_id, [])\n",
    "    \n",
    "    # Create individual records for each sponsor\n",
    "    records = []\n",
    "    for i, sponsor in enumerate(sponsors, 1):\n",
    "        record = {\n",
    "            'video_id': video_id,\n",
    "            f'sponsor_{i}_name': sponsor.get('name'),\n",
    "            f'sponsor_{i}_domain': sponsor.get('domain'),\n",
    "            f'sponsor_{i}_evidence': sponsor.get('evidence')\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "def validate_sponsor_record(record: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate a sponsor record\n",
    "    \n",
    "    Args:\n",
    "        record: Dictionary containing sponsor data\n",
    "        \n",
    "    Returns:\n",
    "        True if record is valid, False otherwise\n",
    "    \"\"\"\n",
    "    required_fields = ['video_id']\n",
    "    sponsor_fields = ['name', 'domain', 'evidence']\n",
    "    \n",
    "    # Check required fields\n",
    "    if not all(field in record for field in required_fields):\n",
    "        return False\n",
    "        \n",
    "    # Check that at least one sponsor exists\n",
    "    has_sponsor = False\n",
    "    i = 1\n",
    "    while f'sponsor_{i}_name' in record:\n",
    "        sponsor_valid = all(\n",
    "            record.get(f'sponsor_{i}_{field}') \n",
    "            for field in sponsor_fields\n",
    "        )\n",
    "        if sponsor_valid:\n",
    "            has_sponsor = True\n",
    "        i += 1\n",
    "        \n",
    "    return has_sponsor\n",
    "\n",
    "def process_sponsor_batch(\n",
    "    videos: List[Dict[str, Any]],\n",
    "    processor,\n",
    "    cache_prefix: str = \"sponsor_detection\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a batch of videos for sponsor detection\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video data dictionaries\n",
    "        processor: LLMProcessor instance\n",
    "        cache_prefix: Prefix for cache keys\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing processed sponsor data\n",
    "    \"\"\"\n",
    "    # Process videos through LLM processor\n",
    "    results = processor.process_batch(\n",
    "        items=videos,\n",
    "        process_fn=process_video_batch,\n",
    "        transform_fn=transform_results,\n",
    "        cache_prefix=cache_prefix\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add metadata\n",
    "    df['processed_timestamp'] = pd.Timestamp.now()\n",
    "    df['cache_prefix'] = cache_prefix\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import concurrent.futures\n",
    "from time import perf_counter\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Global caches\n",
    "url_cache = {}\n",
    "domain_cache = {}\n",
    "first_pass_results = {}  # Cache for videos that found sponsors in first pass\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure function execution time\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = perf_counter()\n",
    "        wrapper.total_time += end - start\n",
    "        wrapper.calls += 1\n",
    "        return result\n",
    "    wrapper.total_time = 0\n",
    "    wrapper.calls = 0\n",
    "    return wrapper\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create a requests session with retries and timeouts\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "@measure_time\n",
    "def expand_url(url):\n",
    "    \"\"\"Expand shortened URLs with robust caching and handling\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "        \n",
    "    if url in url_cache:\n",
    "        return url_cache[url]\n",
    "        \n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "            \n",
    "        session = create_session()\n",
    "        \n",
    "        # Special handling for known URL shorteners\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        if any(service in domain for service in ['bit.ly', 'goo.gl', 'tinyurl']):\n",
    "            response = session.get(\n",
    "                url,\n",
    "                allow_redirects=True,\n",
    "                timeout=10,\n",
    "                headers={'User-Agent': 'Mozilla/5.0'},\n",
    "                stream=True\n",
    "            )\n",
    "        else:\n",
    "            response = session.head(\n",
    "                url,\n",
    "                allow_redirects=True,\n",
    "                timeout=5\n",
    "            )\n",
    "        \n",
    "        final_url = response.url\n",
    "        if isinstance(response, requests.models.Response):\n",
    "            response.close()\n",
    "        \n",
    "        url_cache[url] = final_url\n",
    "        return final_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"URL expansion error for {url}: {e}\")\n",
    "        return url\n",
    "\n",
    "def quick_domain_extract(url):\n",
    "    \"\"\"Extract domain with improved caching\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    if url_lower in domain_cache:\n",
    "        return domain_cache[url_lower]\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url_lower)\n",
    "        domain = parsed.netloc or parsed.path.split('/')[0]\n",
    "        \n",
    "        for prefix in ['www.']:\n",
    "            if domain.startswith(prefix):\n",
    "                domain = domain[len(prefix):]\n",
    "                \n",
    "        domain_cache[url_lower] = domain\n",
    "        return domain\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"Create unified prompt for both passes\"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, row in enumerate(video_batch.iterrows(), 1):\n",
    "        _, video = row\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content):\n",
    "    \"\"\"Process LLM response without URL expansion\"\"\"\n",
    "    try:\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_batch(batch, is_second_pass=False):\n",
    "    \"\"\"Process one batch with two-pass system\"\"\"\n",
    "    try:\n",
    "        # Skip videos that already have sponsors from first pass\n",
    "        if is_second_pass:\n",
    "            batch = batch[~batch['videoId'].isin(first_pass_results.keys())]\n",
    "            if batch.empty:\n",
    "                return {}\n",
    "        \n",
    "        # Use appropriate description length\n",
    "        desc_length = 1500 if is_second_pass else 200\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": create_prompt(batch, desc_length)\n",
    "            }],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        batch_results = process_batch_response(content)\n",
    "        \n",
    "        # Cache first pass results\n",
    "        if not is_second_pass:\n",
    "            first_pass_results.update(batch_results)\n",
    "        \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {'second' if is_second_pass else 'first'} pass: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_videos_parallel(df, batch_size=5, max_workers=3):\n",
    "    \"\"\"Process videos with two-pass system\"\"\"\n",
    "    sponsor_map = {}\n",
    "    batches = [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Processing videos (first pass)\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # First pass - 200 tokens\n",
    "            futures = [executor.submit(process_batch, batch, False) for batch in batches]\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    sponsor_map.update(results)\n",
    "                    pbar.update(batch_size)\n",
    "                except Exception as e:\n",
    "                    print(f\"Batch processing failed: {e}\")\n",
    "    \n",
    "    # Second pass for videos without sponsors\n",
    "    remaining_videos = len(df) - len(first_pass_results)\n",
    "    if remaining_videos > 0:\n",
    "        with tqdm(total=remaining_videos, desc=\"Processing videos (second pass)\") as pbar:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                futures = [executor.submit(process_batch, batch, True) for batch in batches]\n",
    "                \n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    try:\n",
    "                        results = future.result()\n",
    "                        sponsor_map.update(results)\n",
    "                        pbar.update(len(results))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Batch processing failed: {e}\")\n",
    "    \n",
    "    return sponsor_map\n",
    "\n",
    "def expand_sponsor_data(df, sponsor_map):\n",
    "    \"\"\"Expand sponsor data into columns efficiently\"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find max sponsors accounting for all videos\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for all possible sponsors\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def save_checkpoint(df, sponsor_map, current_idx, pass_number):\n",
    "    \"\"\"Save processing checkpoint with pass information\"\"\"\n",
    "    temp_df = df.copy()\n",
    "    temp_df['sponsor_data'] = temp_df['videoId'].map(lambda x: json.dumps(sponsor_map.get(x, [])))\n",
    "    temp_df.to_csv(f'sponsors_checkpoint_pass{pass_number}_{current_idx}.csv', index=False)\n",
    "\n",
    "def print_stats(df, sponsor_map):\n",
    "    \"\"\"Print detailed processing statistics\"\"\"\n",
    "    first_pass_count = len(first_pass_results)\n",
    "    second_pass_count = len(sponsor_map) - first_pass_count\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    print(f\"Videos with sponsors found in first pass (200 tokens): {first_pass_count}\")\n",
    "    print(f\"Additional sponsors found in second pass (1500 tokens): {second_pass_count}\")\n",
    "    print(f\"Total videos with sponsors: {len(sponsor_map)}\")\n",
    "    \n",
    "    # URL expansion stats\n",
    "    if expand_url.calls > 0:\n",
    "        avg_time = expand_url.total_time / expand_url.calls\n",
    "        print(f\"\\nURL Processing:\")\n",
    "        print(f\"Total URLs processed: {expand_url.calls}\")\n",
    "        print(f\"Average processing time: {avg_time:.2f}s per URL\")\n",
    "        print(f\"Cache hits: {len(url_cache)}\")\n",
    "    \n",
    "    # Sponsor distribution\n",
    "    sponsor_counts = [len(sponsors) for sponsors in sponsor_map.values()]\n",
    "    if sponsor_counts:\n",
    "        print(\"\\nSponsor Distribution:\")\n",
    "        print(f\"Average sponsors per video: {sum(sponsor_counts)/len(sponsor_counts):.2f}\")\n",
    "        print(f\"Max sponsors in a video: {max(sponsor_counts)}\")\n",
    "        \n",
    "        count_distribution = pd.Series(sponsor_counts).value_counts().sort_index()\n",
    "        print(\"\\nVideos by sponsor count:\")\n",
    "        for count, videos in count_distribution.items():\n",
    "            print(f\"{count} sponsor(s): {videos} videos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read sample data\n",
    "        df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "        \n",
    "        # Process videos with two-pass system\n",
    "        sponsor_map = process_videos_parallel(\n",
    "            df, \n",
    "            batch_size=10,  # Adjust based on your testing\n",
    "            max_workers=100   # Adjust based on your CPU\n",
    "        )\n",
    "        \n",
    "        # Expand and save sponsor data\n",
    "        final_df = expand_sponsor_data(df, sponsor_map)\n",
    "        final_df.to_csv('sponsors_chunk_8.csv', index=False)\n",
    "        \n",
    "        # Print detailed stats\n",
    "        print_stats(df, sponsor_map)\n",
    "        \n",
    "        # Print sample of found sponsors\n",
    "        print(\"\\nSample Sponsors Found:\")\n",
    "        sample_videos = list(sponsor_map.items())[:3]\n",
    "        for video_id, sponsors in sample_videos:\n",
    "            print(f\"\\nVideo {video_id}:\")\n",
    "            for i, sponsor in enumerate(sponsors, 1):\n",
    "                print(f\"  Sponsor {i}:\")\n",
    "                print(f\"    Name: {sponsor.get('name')}\")\n",
    "                print(f\"    Domain: {sponsor.get('domain')}\")\n",
    "                print(f\"    Evidence: {sponsor.get('evidence')[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/16678 [00:16<25:36:23,  5.53s/it, processed=3, success_rate=0.0%, errors=0, avg_time=4.29s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 288\u001b[0m\n\u001b[1;32m    285\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Process with library\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_videos_with_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msk-cd405682db094b6781f9f815840163d8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    294\u001b[0m final_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[29], line 178\u001b[0m, in \u001b[0;36mprocess_videos_with_library\u001b[0;34m(df, api_key)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Process first pass\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m first_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_first_pass\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Process second pass only for remaining videos\u001b[39;00m\n\u001b[1;32m    184\u001b[0m remaining \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    185\u001b[0m     v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m first_pass_results\n\u001b[1;32m    187\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/multi_llm/llm_processor/multi_processing/processor.py:185\u001b[0m, in \u001b[0;36mLLMProcessor.process_batch\u001b[0;34m(self, items, process_fn, transform_fn, cache_prefix, use_cache, output_path)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    179\u001b[0m     fn \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_with_retry,\n\u001b[1;32m    181\u001b[0m         process_fn\u001b[38;5;241m=\u001b[39mprocess_fn,\n\u001b[1;32m    182\u001b[0m         cache_prefix\u001b[38;5;241m=\u001b[39mcache_prefix,\n\u001b[1;32m    183\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache\n\u001b[1;32m    184\u001b[0m     )\n\u001b[0;32m--> 185\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(fn, batch_data))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Transform results if needed\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_fn:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sponsor_processor.py\n",
    "\n",
    "from multi_processing.processor import LLMProcessor, ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"Create unified prompt for both passes\"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(video_batch, 1):\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content):\n",
    "    \"\"\"Process LLM response without URL expansion\"\"\"\n",
    "    try:\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return {}\n",
    "\n",
    "def expand_sponsor_data(df, sponsor_map):\n",
    "    \"\"\"Expand sponsor data into columns efficiently\"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find max sponsors accounting for all videos\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for all possible sponsors\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def print_stats(df, sponsor_map):\n",
    "    \"\"\"Print detailed processing statistics\"\"\"\n",
    "    first_pass_count = len(first_pass_results)\n",
    "    second_pass_count = len(sponsor_map) - first_pass_count\n",
    "    \n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total videos processed: {len(df)}\")\n",
    "    print(f\"Videos with sponsors found in first pass (200 tokens): {first_pass_count}\")\n",
    "    print(f\"Additional sponsors found in second pass (1500 tokens): {second_pass_count}\")\n",
    "    print(f\"Total videos with sponsors: {len(sponsor_map)}\")\n",
    "    \n",
    "    # Sponsor distribution\n",
    "    sponsor_counts = [len(sponsors) for sponsors in sponsor_map.values()]\n",
    "    if sponsor_counts:\n",
    "        print(\"\\nSponsor Distribution:\")\n",
    "        print(f\"Average sponsors per video: {sum(sponsor_counts)/len(sponsor_counts):.2f}\")\n",
    "        print(f\"Max sponsors in a video: {max(sponsor_counts)}\")\n",
    "        \n",
    "        count_distribution = pd.Series(sponsor_counts).value_counts().sort_index()\n",
    "        print(\"\\nVideos by sponsor count:\")\n",
    "        for count, videos in count_distribution.items():\n",
    "            print(f\"{count} sponsor(s): {videos} videos\")\n",
    "\n",
    "# New wrapper for library integration\n",
    "def process_videos_with_library(df: pd.DataFrame, api_key: str):\n",
    "    \"\"\"Process videos using library's built-in parallel processing\"\"\"\n",
    "    \n",
    "    # Initialize basic config\n",
    "    config = ProcessorConfig(\n",
    "    cache_enabled=False,     # No disk cache\n",
    "    rate_limit=0.0,          # No forced sleep between calls\n",
    "    max_retries=1,           # Or 2, if you rarely fail\n",
    "    batch_size=1,            # Not crucial if you're doing item-level concurrency anyway\n",
    "    max_workers=100,         # If your system can handle it\n",
    "    fail_fast=True,\n",
    "    # ... etc.\n",
    ")\n",
    "    \n",
    "    client = DeepSeekClient(api_key=api_key)\n",
    "    processor = LLMProcessor(client, config)\n",
    "    \n",
    "    # Track first pass results\n",
    "    first_pass_results = {}\n",
    "    \n",
    "    # Define processing functions\n",
    "    def process_first_pass(video):\n",
    "        \"\"\"First pass with short descriptions\"\"\"\n",
    "        response = client.call_api(\n",
    "            create_prompt([video], desc_length=200)\n",
    "        )\n",
    "        if response['success']:\n",
    "            results = process_batch_response(response['content'])\n",
    "            first_pass_results.update(results)\n",
    "        return results\n",
    "\n",
    "    # Process first pass\n",
    "    first_results = processor.process_batch(\n",
    "        items=df.to_dict('records'),\n",
    "        process_fn=process_first_pass\n",
    "    )\n",
    "    \n",
    "    # Process second pass only for remaining videos\n",
    "    remaining = [\n",
    "        v for v in df.to_dict('records')\n",
    "        if v['videoId'] not in first_pass_results\n",
    "    ]\n",
    "    \n",
    "    def process_second_pass(video):\n",
    "        \"\"\"Second pass with longer descriptions\"\"\"\n",
    "        response = client.call_api(\n",
    "            create_prompt([video], desc_length=1500)\n",
    "        )\n",
    "        if response['success']:\n",
    "            return process_batch_response(response['content'])\n",
    "        return {}\n",
    "\n",
    "    second_results = processor.process_batch(\n",
    "        items=remaining,\n",
    "        process_fn=process_second_pass\n",
    "    )\n",
    "    \n",
    "    # Combine results and expand\n",
    "    all_results = {**first_results, **second_results}\n",
    "    return expand_sponsor_data(df, all_results)\n",
    "\n",
    "    \n",
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "\n",
    "class Telemetry:\n",
    "    def __init__(self):\n",
    "        self.timings = defaultdict(list)\n",
    "        self.counts = defaultdict(int)\n",
    "        \n",
    "    def measure(self, operation):\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start = perf_counter()\n",
    "                result = func(*args, **kwargs)\n",
    "                duration = perf_counter() - start\n",
    "                self.timings[operation].append(duration)\n",
    "                self.counts[operation] += 1\n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def print_stats(self):\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        for op, times in self.timings.items():\n",
    "            avg_time = sum(times) / len(times)\n",
    "            total_time = sum(times)\n",
    "            print(f\"\\n{op}:\")\n",
    "            print(f\"  Count: {self.counts[op]}\")\n",
    "            print(f\"  Average time: {avg_time:.2f}s\")\n",
    "            print(f\"  Total time: {total_time:.2f}s\")\n",
    "            print(f\"  % of total time: {(total_time/sum(sum(t) for t in self.timings.values()))*100:.1f}%\")\n",
    "\n",
    "# Add to your processor\n",
    "telemetry = Telemetry()\n",
    "\n",
    "@telemetry.measure(\"API Call\")\n",
    "def process_batch(batch, is_second_pass=False):\n",
    "    \"\"\"Process batch with timing\"\"\"\n",
    "    try:\n",
    "        if is_second_pass:\n",
    "            batch = [v for v in batch if v['videoId'] not in first_pass_results]\n",
    "            if not batch:\n",
    "                return {}\n",
    "        \n",
    "        desc_length = 1500 if is_second_pass else 200\n",
    "        \n",
    "        # Measure prompt creation\n",
    "        start = perf_counter()\n",
    "        prompt = create_prompt(batch, desc_length)\n",
    "        telemetry.timings[\"prompt_creation\"].append(perf_counter() - start)\n",
    "        \n",
    "        # Measure API call\n",
    "        start = perf_counter()\n",
    "        response = client.call_api(prompt)\n",
    "        telemetry.timings[\"pure_api_call\"].append(perf_counter() - start)\n",
    "        \n",
    "        if not response['success']:\n",
    "            return {}\n",
    "            \n",
    "        # Measure response processing    \n",
    "        start = perf_counter()\n",
    "        batch_results = process_batch_response(response['content'])\n",
    "        telemetry.timings[\"response_processing\"].append(perf_counter() - start)\n",
    "        \n",
    "        if not is_second_pass:\n",
    "            first_pass_results.update(batch_results)\n",
    "            \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {'second' if is_second_pass else 'first'} pass: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read data\n",
    "        df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "        \n",
    "        # Process with library\n",
    "        final_df = process_videos_with_library(\n",
    "            df,\n",
    "            api_key='sk-cd405682db094b6781f9f815840163d8'\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        final_df.to_csv('sponsor_results.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running first pass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  43%|████▎     | 434/1000 [00:04<00:01, 380.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title and description, there is no explicit mention of brand sponsorships or promotional content. Without access to the actual video content or additional details, I cannot identify any sponsorships. Therefore, the JSON object will reflect that no sponsors were found for this video.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"qbh6C4tlEfg\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "``` \n",
      "\n",
      "If you have access to the video content or additional details (e.g., captions, comments, or timestamps), I can reanalyze it for potential sponsorships. Let me know!\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video details, there is no explicit mention of brand sponsorships, integrations, or partnerships in the title, description, or context of the video. The content appears to focus on a dramatic incident in San Diego, and there is no promotional intent or brand-related evidence provided.\n",
      "\n",
      "Here is the JSON object reflecting the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"oeNB8xZ2GRk\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If additional information or context becomes available (e.g., within the video itself), the analysis can be updated accordingly. Let me know if you'd like further assistance!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  63%|██████▎   | 627/1000 [00:05<00:00, 428.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title and description, there is no explicit mention of brand sponsorships, integrations, or partnerships. The video appears to focus on analyzing crypto influencers and does not directly reference any brands with promotional intent. Therefore, no sponsorships can be identified from the given information.\n",
      "\n",
      "Here is the JSON object reflecting this analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"rHILmX0qBKM\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If additional details or context from the video content itself are available, further analysis could be conducted to identify potential sponsorships.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and look for direct mentions of brands with promotional intent, sponsored integrations, brand deals, or partnerships. Since I cannot directly access or watch the video, I cannot provide a definitive analysis of its sponsorships.\n",
      "\n",
      "However, if you provide a transcript or specific timestamps where sponsorships are mentioned, I can help identify and format the sponsorships into the requested JSON structure. Let me know how you'd like to proceed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  74%|███████▍  | 738/1000 [00:05<00:00, 467.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video details (ID, title, and description), there is no explicit evidence of brand sponsorships or promotional content. The description does not mention any brands, partnerships, or sponsored integrations. Therefore, no sponsorships can be identified for this video.\n",
      "\n",
      "Here is the JSON object reflecting this analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"yuCEfIuE0os\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "``` \n",
      "\n",
      "If additional details or context from the video itself (e.g., in-video mentions or visuals) become available, the analysis can be updated accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  87%|████████▋ | 872/1000 [00:05<00:00, 381.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title and description, there is no explicit mention of brand sponsorships, integrations, or partnerships. The video appears to focus on discussing Aspen Pittman's legacy and the repair of a Fender Pro Reverb amplifier, with no promotional intent for any brands. Therefore, no sponsorships are identified for this video.\n",
      "\n",
      "Here is the JSON object reflecting this analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"CvTRANw9l6w\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title, description, and guidelines, there is no explicit evidence of brand sponsorships or promotional content in the metadata provided for **VIDEO 1**. The description focuses on the storyline of the drama and does not mention any brands or sponsorships.\n",
      "\n",
      "Here is the JSON object reflecting this analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"xjFRMeagT9s\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If additional information or context from the video itself (e.g., in-video mentions, credits, or overlays) becomes available, the analysis can be updated accordingly.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video description, there is no clear evidence of brand sponsorships or direct mentions of brands with promotional intent. The description primarily includes links to the creator's merchandise, social media, and YouTube channel, which do not qualify as brand sponsorships under the given guidelines.\n",
      "\n",
      "Here is the JSON object reflecting the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"1hbU250S1-M\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If additional context or details from the video content itself (e.g., spoken mentions or visual integrations) are available, further analysis could be conducted. Let me know if you'd like to provide more information!\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and look for direct mentions of brands with promotional intent, sponsored integrations, brand deals, or partnerships. Since I cannot watch the video directly, I cannot provide a definitive analysis of its sponsorships. However, if you can provide a transcript or specific timestamps where sponsorships are mentioned, I can help identify the sponsors and format the JSON object accordingly.\n",
      "\n",
      "For now, here’s a placeholder JSON structure based on the information provided:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"htIuynddkqQ\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If you can provide more details or a transcript, I can update this with accurate sponsorship information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  91%|█████████ | 911/1000 [00:06<00:00, 295.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title, description, and guidelines, there is no explicit evidence of brand sponsorships in the video metadata. The content appears to focus on commentary about the \"Ninja Sentai Kakuranger 30 Years After Special\" and does not mention any brands or promotional intent.\n",
      "\n",
      "Here is the JSON object reflecting the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"qrnQLez1a6s\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If there are additional details or transcripts from the video that might reveal sponsorships, please provide them for further analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  94%|█████████▍| 944/1000 [00:06<00:00, 244.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided information, the video description is incomplete and does not contain sufficient details to identify any sponsorships. Without explicit mentions of brands or promotional intent in the description or title, it is not possible to determine any sponsorships for this video.\n",
      "\n",
      "Here is the JSON object reflecting the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"oc-OL7Z7cTo\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If additional details or the full description of the video are provided, I can reanalyze and update the JSON object accordingly.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title and description, there is no explicit evidence of brand sponsorships or promotional intent for any specific brand. The video appears to focus on painting techniques for models, specifically mentioning \"Rangers and Wehrmacht from the Warlord Games Bolt Action,\" but this seems to be a reference to the models rather than a sponsorship or promotional deal.\n",
      "\n",
      "Since no direct sponsorship evidence is found in the provided information, the JSON object will reflect that no sponsors were identified:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"lX-zX85jTyc\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  97%|█████████▋| 972/1000 [00:06<00:00, 154.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title and description, there is no explicit mention of brand sponsorships, integrations, or partnerships. The content appears to focus on gameplay mechanics and updates for *Genshin Impact*, specifically discussing elemental reactions in patch 5.2. There is no evidence of promotional intent or brand mentions in the provided text.\n",
      "\n",
      "Here is the JSON object reflecting the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"_fbe7Stbba4\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If additional details or context from the video itself (e.g., in-video mentions or visuals) are available, further analysis could be conducted. However, based on the provided metadata, no sponsorships are identified.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video title and description, there is no explicit mention of brand sponsorships, integrations, or partnerships. The content appears to focus solely on gameplay mechanics and builds in Destiny 2, specifically discussing Tinasha's Mastery and its synergies with Chill Clip. Without additional information or access to the video's full content (e.g., spoken mentions, visual overlays, or end screens), no sponsorships can be identified.\n",
      "\n",
      "Here is the JSON object reflecting this analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"1cxfbkmTRd8\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "``` \n",
      "\n",
      "If you have access to the full video content or additional details (e.g., captions, visuals, or end screens), I can reanalyze for potential sponsorships.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to review the video content, description, and any visible or mentioned brand integrations. However, since I cannot directly access or view the video (ID: 2NLrwtq8o8A), I can only analyze the provided description for potential sponsorships.\n",
      "\n",
      "Based on the description provided, there is no explicit mention of any brand sponsorships, integrations, or partnerships. The description focuses on the song's theme and the artist's journey, without referencing any brands or promotional content.\n",
      "\n",
      "Here is the JSON object based on the available information:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"2NLrwtq8o8A\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If there are sponsorships in the video itself (e.g., visual logos, verbal mentions, or other integrations), those would need to be identified by reviewing the video content directly. Let me know if you have additional details or need further assistance!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens):  99%|█████████▉| 994/1000 [00:07<00:00, 68.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and examine its description for any direct mentions of brands with promotional intent. Since I cannot directly access or watch the video, I will rely on the provided title and description to infer potential sponsorships. However, the description provided does not explicitly mention any brands or sponsorships.\n",
      "\n",
      "If you have access to the video or its transcript, you can look for direct mentions of brands with promotional intent, such as:\n",
      "- Verbal mentions of sponsors during the video.\n",
      "- Brand logos or product placements.\n",
      "- Links or discount codes provided in the description.\n",
      "\n",
      "Based on the information provided, here is the JSON object:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"4k841lQ4ZQs\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If you can provide more details or a transcript of the video, I can refine this analysis further.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and review its description for any direct mentions of brands with promotional intent. Since I cannot directly access or watch the video, I will rely on the provided description and title to infer potential sponsorships. However, the description provided does not explicitly mention any brands or sponsorships.\n",
      "\n",
      "Based on the given information, there is no evidence of brand sponsorships in the video description or title. Therefore, the JSON object would reflect no sponsorships found.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"kexGkZWo6wg\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If you have access to the video and can provide additional details or text snippets from the video itself, I can re-evaluate for potential sponsorships.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and examine its description for any direct mentions of brands with promotional intent. However, since I cannot directly access or watch videos, I can only analyze the provided metadata (title, description, etc.).\n",
      "\n",
      "Based on the provided description for **VIDEO 1 (ID: FOlxCQCb58I)**, there is no explicit mention of brand sponsorships, partnerships, or promotional content. The description primarily includes links to Mike Finoia's personal website, YouTube channels, and a mention of a live show, but no brands or sponsors are directly referenced.\n",
      "\n",
      "Here is the JSON object based on the available information:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"FOlxCQCb58I\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If there are sponsorships mentioned within the video itself (e.g., verbal mentions or visual integrations), those would need to be identified by watching the video. Let me know if you have additional details or transcripts to analyze!\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and review its description for any direct mentions of brands with promotional intent, sponsored integrations, brand deals, or partnerships. However, since I cannot directly access or watch videos, I can only analyze the provided description for potential sponsorships.\n",
      "\n",
      "Based on the description provided for **VIDEO 1**, there is no explicit mention of any brands or sponsorships. The description focuses on the theme of the video and does not include any promotional content or brand partnerships.\n",
      "\n",
      "Here is the JSON object based on the available information:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"Q5qiXjOj_Uc\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If there are sponsorships in the video itself (e.g., verbal mentions, visual branding, or other integrations), those would need to be identified by watching the video. Let me know if you have additional details or transcripts to analyze!\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and review its description for any direct mentions of brands with promotional intent. Since I cannot directly access or watch the video, I will provide a template JSON structure based on the information you provided. You can fill in the details after reviewing the video.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"q_ZSs57pPn4\",\n",
      "            \"sponsors\": [\n",
      "                {\n",
      "                    \"name\": \"Brand name\",\n",
      "                    \"domain\": \"branddomain.com\",\n",
      "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "### Steps to Identify Sponsorships:\n",
      "1. **Watch the Video**: Look for verbal or visual mentions of brands, especially those with promotional intent (e.g., discounts, special offers, or explicit sponsorship mentions).\n",
      "2. **Check the Description**: Review the video description for any sponsored links, brand partnerships, or promotional codes.\n",
      "3. **Identify Main Domains**: Use the main company domain (e.g., `brandname.com`) for each sponsor, ignoring promo-specific URLs.\n",
      "4. **Provide Evidence**: Include a direct quote or text snippet from the video or description that confirms the sponsorship.\n",
      "\n",
      "If you provide more details or specific text from the video or description, I can help refine the JSON object further.\n",
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and review its description, title, and any visible branding or mentions within the content itself. However, since I cannot directly access or watch videos, I can only analyze the provided metadata (title and description) for potential sponsorships.\n",
      "\n",
      "Based on the provided information for **VIDEO 1** (ID: `ie38sgtVRqA`), there is no explicit mention of brand sponsorships in the title or description. The description focuses on the song's themes and does not include any promotional content or brand mentions.\n",
      "\n",
      "If there are sponsorships in the video itself (e.g., visual branding, verbal mentions, or end-screen promotions), they are not evident from the metadata alone. To accurately identify sponsorships, the video content would need to be reviewed.\n",
      "\n",
      "Here is the JSON object based on the available information:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"ie38sgtVRqA\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If you can provide additional details or context from the video (e.g., transcript, visual cues, or specific timestamps), I can refine the analysis further.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First pass (200 tokens): 100%|██████████| 1000/1000 [00:09<00:00, 101.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: To analyze the video for brand sponsorships, I would need to watch the video and examine its description, title, and any visible or audible mentions of brands. However, since I cannot directly access or watch videos, I can only analyze the provided text (title and description) for potential sponsorships.\n",
      "\n",
      "Based on the provided information:\n",
      "\n",
      "1. **Video ID**: `aMEoQSPRRh4`\n",
      "2. **Title**: \"Max Verstappen's Pole Lap | 2024 Qatar Grand Prix | Pirelli\"\n",
      "3. **Description**: \"Ride onboard with Max Verstappen as he secures pole position in Qatar, his first pole in 12 races! Ride down The Strip on board with George Russell as the Brit storms to pole position at the end of ...\"\n",
      "\n",
      "From the title, the brand **Pirelli** is explicitly mentioned, which is a well-known tire manufacturer and a sponsor in Formula 1. The description does not provide additional sponsorship details.\n",
      "\n",
      "Here is the JSON object based on the analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"aMEoQSPRRh4\",\n",
      "            \"sponsors\": [\n",
      "                {\n",
      "                    \"name\": \"Pirelli\",\n",
      "                    \"domain\": \"pirelli.com\",\n",
      "                    \"evidence\": \"Title: 'Max Verstappen's Pole Lap | 2024 Qatar Grand Prix | Pirelli'\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "If there are additional sponsors in the video itself (e.g., logos, verbal mentions, or other integrations), they would need to be identified by watching the video. Let me know if you have more details or need further assistance!\n",
      "\n",
      "Running second pass for 24 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Second pass (1500 tokens):  67%|██████▋   | 16/24 [00:03<00:00,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing response JSON: Expecting value: line 1 column 1 (char 0), Content: Based on the provided video description and content, there is no direct evidence of brand sponsorships, integrations, or partnerships in the video. The description primarily includes sources, music credits, and chapter timestamps, but no promotional content or brand mentions. Therefore, the JSON object will reflect that no sponsorships were identified.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"video_sponsors\": [\n",
      "        {\n",
      "            \"video_id\": \"wkPU4xCV3mU\",\n",
      "            \"sponsors\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Second pass (1500 tokens): 100%|██████████| 24/24 [00:08<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Total videos: 1000\n",
      "First pass sponsors: 976\n",
      "Total sponsors found: 999\n",
      "Processing complete! Results saved to sponsor_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sponsor_processor.py\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "from multi_processing.llm_client import DeepSeekClient\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#########################################\n",
    "# Helper Functions\n",
    "#########################################\n",
    "\n",
    "def create_prompt(video_batch, desc_length=200):\n",
    "    \"\"\"\n",
    "    Create a unified prompt for sponsor extraction.\n",
    "    Each item in video_batch is a dict with keys: videoId, title, description, etc.\n",
    "    \"\"\"\n",
    "    videos_text = \"\"\n",
    "    for i, video in enumerate(video_batch, 1):\n",
    "        description = video['description'][:desc_length] + \"...\" if len(video['description']) > desc_length else video['description']\n",
    "        videos_text += f\"\"\"VIDEO {i}:\n",
    "ID: {video['videoId']}\n",
    "Title: {video['title']}\n",
    "Description: {description}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {len(video_batch)} videos for brand sponsorships.\n",
    "\n",
    "{videos_text}\n",
    "Return a JSON object with video IDs mapping to their sponsors:\n",
    "{{\n",
    "    \"video_sponsors\": [\n",
    "        {{\n",
    "            \"video_id\": \"the_video_id\",\n",
    "            \"sponsors\": [\n",
    "                {{\n",
    "                    \"name\": \"Brand name (e.g., 'Surfshark' not 'surfshark vpn')\",\n",
    "                    \"domain\": \"Main company domain (e.g., 'surfshark.com' not promo URLs)\",\n",
    "                    \"evidence\": \"Exact text snippet showing sponsorship\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Guidelines for identifying sponsorships:\n",
    "- Look for direct mentions of brands with promotional intent\n",
    "- Include sponsored integrations, brand deals, partnerships\n",
    "- Use main company domains (e.g., 'nordvpn.com' not 'nordvpn.com/creator')\n",
    "- For each brand found, use their official domain regardless of promo links\n",
    "- Include multiple sponsors if present\n",
    "- Ignore: merch, generic affiliate links, social media, donations, self promo\n",
    "\n",
    "Examples of correct domain mapping:\n",
    "- Surfshark promo link -> surfshark.com\n",
    "- Nord VPN creator link -> nordvpn.com\n",
    "- Skillshare special offer -> skillshare.com\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_batch_response(content: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Parse the JSON returned by the LLM and extract a dict:\n",
    "    { video_id: [ {name, domain, evidence}, ... ] }\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # If the LLM wraps JSON in code fences, strip them out\n",
    "        if content.startswith(\"```\"):\n",
    "            json_start = content.find(\"{\")\n",
    "            json_end = content.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                content = content[json_start:json_end]\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        batch_results = {}\n",
    "        #print(f\"Parsed JSON: {result}\")  # Debug print\n",
    "\n",
    "        \n",
    "        if 'video_sponsors' in result:\n",
    "            for video_data in result['video_sponsors']:\n",
    "                video_id = video_data['video_id']\n",
    "                sponsors = video_data.get('sponsors', [])\n",
    "                batch_results[video_id] = sponsors\n",
    "                \n",
    "        return batch_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response JSON: {e}, Content: {content}\")\n",
    "        return {}\n",
    "\n",
    "def expand_sponsor_data(df: pd.DataFrame, sponsor_map: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add columns for each sponsor (name, domain, evidence) to the original DataFrame.\n",
    "    \"\"\"\n",
    "    df['sponsor_data'] = df['videoId'].map(lambda x: sponsor_map.get(x, []))\n",
    "    \n",
    "    # Find the maximum number of sponsors that any video has\n",
    "    max_sponsors = max((len(sponsors) for sponsors in sponsor_map.values()), default=0)\n",
    "    \n",
    "    all_sponsor_rows = []\n",
    "    for video_id in df['videoId']:\n",
    "        sponsors = sponsor_map.get(video_id, [])\n",
    "        row_sponsors = []\n",
    "        # For each sponsor slot, create columns\n",
    "        for i in range(max_sponsors):\n",
    "            if i < len(sponsors):\n",
    "                sponsor = sponsors[i]\n",
    "                row_sponsors.extend([\n",
    "                    sponsor.get('name', None),\n",
    "                    sponsor.get('domain', None),\n",
    "                    sponsor.get('evidence', None)\n",
    "                ])\n",
    "            else:\n",
    "                row_sponsors.extend([None, None, None])\n",
    "        all_sponsor_rows.append(row_sponsors)\n",
    "    \n",
    "    # Create column names for the sponsor slots\n",
    "    column_names = []\n",
    "    for i in range(max_sponsors):\n",
    "        column_names.extend([\n",
    "            f\"sponsor_{i+1}_name\",\n",
    "            f\"sponsor_{i+1}_domain\",\n",
    "            f\"sponsor_{i+1}_evidence\"\n",
    "        ])\n",
    "\n",
    "    sponsor_expanded_df = pd.DataFrame(all_sponsor_rows, columns=column_names)\n",
    "    \n",
    "    # Combine with the original data\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    sponsor_expanded_df.reset_index(drop=True, inplace=True)\n",
    "    final_df = pd.concat([df, sponsor_expanded_df], axis=1)\n",
    "    return final_df\n",
    "\n",
    "#########################################\n",
    "# Two-Pass Sponsor Processing\n",
    "#########################################\n",
    "\n",
    "def process_videos_with_library(df: pd.DataFrame, api_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Two-pass sponsor extraction using the LLMProcessor\"\"\"\n",
    "\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        enable_batch_prompts=True,  # Enable batching\n",
    "        batch_size=1,             # Process 10 videos at a time\n",
    "        max_workers=1000,\n",
    "        fail_fast=False\n",
    "    )\n",
    "    \n",
    "    client = DeepSeekClient(api_key=api_key, model=\"deepseek-chat\")\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    # Track all results across both passes\n",
    "    all_sponsors = {}\n",
    "    first_pass_sponsors = {}\n",
    "\n",
    "    def process_first_pass(batch: List[Dict]) -> Dict:\n",
    "        \"\"\"Process a batch of videos with short descriptions\"\"\"\n",
    "        prompt = create_prompt(batch, desc_length=200)\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        if not response.get('success'):\n",
    "            return {}\n",
    "            \n",
    "        sponsors = process_batch_response(response['content'])\n",
    "        # Update our tracking dict\n",
    "        first_pass_sponsors.update(sponsors)\n",
    "        return sponsors\n",
    "\n",
    "    # First pass\n",
    "    print(\"\\nRunning first pass...\")\n",
    "    first_pass_results = processor.process_batch(\n",
    "        items=df.to_dict('records'),\n",
    "        process_fn=process_first_pass,\n",
    "        desc=\"First pass (200 tokens)\"\n",
    "    )\n",
    "    \n",
    "    # Combine first pass results\n",
    "    for result in first_pass_results:\n",
    "        all_sponsors.update(result)\n",
    "\n",
    "    # Second pass only for videos without sponsors\n",
    "    remaining = [\n",
    "        video for video in df.to_dict('records')\n",
    "        if video['videoId'] not in first_pass_sponsors\n",
    "    ]\n",
    "\n",
    "    if remaining:\n",
    "        print(f\"\\nRunning second pass for {len(remaining)} videos...\")\n",
    "        \n",
    "        def process_second_pass(batch: List[Dict]) -> Dict:\n",
    "            \"\"\"Process a batch with longer descriptions\"\"\"\n",
    "            prompt = create_prompt(batch, desc_length=1500)\n",
    "            response = client.call_api(prompt)\n",
    "            \n",
    "            if not response.get('success'):\n",
    "                return {}\n",
    "                \n",
    "            return process_batch_response(response['content'])\n",
    "\n",
    "        second_pass_results = processor.process_batch(\n",
    "            items=remaining,\n",
    "            process_fn=process_second_pass,\n",
    "            desc=\"Second pass (1500 tokens)\"\n",
    "        )\n",
    "        \n",
    "        # Combine second pass results\n",
    "        for result in second_pass_results:\n",
    "            all_sponsors.update(result)\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total videos: {len(df)}\")\n",
    "    print(f\"First pass sponsors: {len(first_pass_sponsors)}\")\n",
    "    print(f\"Total sponsors found: {len(all_sponsors)}\")\n",
    "\n",
    "    # Expand and return\n",
    "    return expand_sponsor_data(df, all_sponsors)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Example Main\n",
    "#########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load your CSV\n",
    "        df = pd.read_csv(\"/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv\", nrows=1000)\n",
    "        \n",
    "        # Put your actual DeepSeek/LLM API key here\n",
    "        API_KEY = \"sk-cd405682db094b6781f9f815840163d8\"\n",
    "\n",
    "        # Run the two-pass sponsor extraction\n",
    "        final_df = process_videos_with_library(df, API_KEY)\n",
    "\n",
    "        # Save results\n",
    "        final_df.to_csv(\"sponsor_results.csv\", index=False)\n",
    "        \n",
    "        print(\"Processing complete! Results saved to sponsor_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running sponsor processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_video_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m video_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process videos with our framework\u001b[39;00m\n\u001b[1;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mprocess_batch(\n\u001b[1;32m     10\u001b[0m     items\u001b[38;5;241m=\u001b[39mvideo_data,\n\u001b[0;32m---> 11\u001b[0m     process_fn\u001b[38;5;241m=\u001b[39m\u001b[43mprocess_video_batch\u001b[49m,\n\u001b[1;32m     12\u001b[0m     transform_fn\u001b[38;5;241m=\u001b[39mtransform_results,\n\u001b[1;32m     13\u001b[0m     cache_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_extraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msponsor_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 6. Analyze Results\u001b[39;00m\n\u001b[1;32m     18\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_video_batch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Load and Process Data\n",
    "# Load your video data\n",
    "df = pd.read_csv('/Users/parthkocheta/Documents/sponsorFind/sponsorFind/chunk_8_of_245.csv')\n",
    "\n",
    "# Convert DataFrame rows to list of dicts\n",
    "video_data = df.to_dict('records')\n",
    "\n",
    "# Process videos with our framework\n",
    "results = processor.process_batch(\n",
    "    items=video_data,\n",
    "    process_fn=process_video_batch,\n",
    "    transform_fn=transform_results,\n",
    "    cache_prefix='sponsor_extraction',\n",
    "    output_path='sponsor_results.csv'\n",
    ")\n",
    "\n",
    "# 6. Analyze Results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nProcessing Statistics:\")\n",
    "print(f\"Total videos processed: {len(df)}\")\n",
    "print(f\"Videos with sponsors found: {len(results_df)}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample Sponsor Results:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# 7. Check Processing Metrics\n",
    "import json\n",
    "with open(\"metrics.json\", 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"\\nProcessing Metrics:\")\n",
    "print(f\"Total processing time: {metrics['total_time']:.2f} seconds\")\n",
    "print(f\"Average time per item: {metrics['avg_process_time']:.2f} seconds\")\n",
    "print(f\"Cache hits: {metrics['cache_hits']}\")\n",
    "print(f\"Total errors: {metrics['errors']}\")\n",
    "\n",
    "# 8. Additional Analysis\n",
    "if results_df.empty:\n",
    "    print(\"No results found\")\n",
    "else:\n",
    "    # Get sponsor frequency\n",
    "    sponsor_cols = [col for col in results_df.columns if 'sponsor_' in col and 'name' in col]\n",
    "    all_sponsors = results_df[sponsor_cols].values.flatten()\n",
    "    sponsor_counts = pd.Series(all_sponsors).value_counts().dropna()\n",
    "\n",
    "    print(\"\\nTop Sponsors:\")\n",
    "    print(sponsor_counts.head())\n",
    "\n",
    "# 9. Save Final Results\n",
    "results_df.to_csv('final_sponsor_analysis.csv', index=False)\n",
    "print(\"\\nResults saved to 'final_sponsor_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 200/200 [00:04<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library] Processed 200 items in 4.24 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw concurrency: 100%|██████████| 200/200 [00:04<00:00, 47.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Concurrency] Processed 200 items in 4.21 seconds.\n",
      "Library results: 200 items.\n",
      "Raw concurrency results: 200 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_processor_speed.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# 1) Fake LLMClient to Simulate API Calls\n",
    "########################################\n",
    "\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class FakeLLMClient(BaseLLMClient):\n",
    "    \"\"\"\n",
    "    A fake client that simulates a 100-300ms \"API call\" time.\n",
    "    No real network usage, just time.sleep().\n",
    "    \"\"\"\n",
    "\n",
    "    def call_api(self, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        # Sleep a random time to simulate latency\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        # Return a pretend success payload\n",
    "        return {\n",
    "            \"content\": f\"Fake response for prompt: {prompt[:30]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def validate_response(self, response: Dict[str, Any]) -> bool:\n",
    "        return response.get(\"success\", False)\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "\n",
    "def generate_fake_dataset(num_items: int = 2000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random text data.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=100))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) The \"process function\" we apply\n",
    "########################################\n",
    "\n",
    "def process_item_with_fake_llm(item: Dict[str, Any], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a single item, call the fake LLM API and return a small result dict.\n",
    "    \"\"\"\n",
    "    prompt = f\"Process text: {item['text']}\"\n",
    "    response = client.call_api(prompt)\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"content\": response[\"content\"],  # from the fake LLM\n",
    "        \"success\": response[\"success\"]\n",
    "    }\n",
    "\n",
    "def run_sequential_no_concurrency(dataset, client):\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Process text: {item['text']}\"\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"content\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        })\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(results)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "########################################\n",
    "# 4) Testing with the LLMProcessor Library\n",
    "########################################\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "def run_with_library(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use your LLMProcessor with concurrency, no caching, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,  # no disk caching\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=1,         # item-level concurrency\n",
    "        fail_fast=False\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()  # Our fake client\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        return process_item_with_fake_llm(item, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    results = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\",   # not used if cache is disabled\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Library] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 5) Testing with Raw Concurrency (ThreadPool)\n",
    "########################################\n",
    "\n",
    "def run_with_raw_concurrency(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use a plain ThreadPoolExecutor approach, no library overhead.\n",
    "    \"\"\"\n",
    "    client = FakeLLMClient()\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=\"Raw concurrency\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in dataset:\n",
    "                futures.append(executor.submit(process_item_with_fake_llm, item, client))\n",
    "            \n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                res = f.result()\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Raw Concurrency] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 6) Main Comparison\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate a sample dataset\n",
    "    data_size = 200  # adjust to see bigger difference\n",
    "    dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "    # 2) Run with library\n",
    "    library_results = run_with_library(dataset, max_workers=10)\n",
    "\n",
    "    # 3) Run with raw concurrency\n",
    "    raw_results = run_with_raw_concurrency(dataset, max_workers=10)\n",
    "\n",
    "    control_results = run_sequential_no_concurrency(dataset, client)\n",
    "\n",
    "    # 4) Quick check\n",
    "    # Validate that we got the same number of results\n",
    "    print(f\"Library results: {len(library_results)} items.\")\n",
    "    print(f\"Raw concurrency results: {len(raw_results)} items.\")\n",
    "    \n",
    "    # If you want to confirm the outputs are consistent, you can compare them\n",
    "    # but here we only compare time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential (control): 100%|██████████| 500/500 [01:41<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Control] Processed 500 items sequentially in 101.50 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 500/500 [00:01<00:00, 416.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library Item-Level] Processed 500 items in 1.21 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Library Batch Mode: 100%|██████████| 50/50 [00:00<00:00, 163.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Library Batch Mode] Processed 500 items in 0.31 seconds (sub-batch size=10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw concurrency: 100%|██████████| 500/500 [00:01<00:00, 412.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw Concurrency] Processed 500 items in 1.22 seconds.\n",
      "\n",
      "Control results: 500 items.\n",
      "Library item-level results: 500 items.\n",
      "Library batch-mode results: 500 items.\n",
      "Raw concurrency results: 500 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_processor_speed.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Dict, Any, List\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# 1) Fake LLMClient to Simulate API Calls\n",
    "########################################\n",
    "\n",
    "from multi_processing.llm_client import BaseLLMClient\n",
    "\n",
    "class FakeLLMClient(BaseLLMClient):\n",
    "    \"\"\"\n",
    "    A fake client that simulates a 100-300ms \"API call\" time.\n",
    "    No real network usage, just time.sleep().\n",
    "    \"\"\"\n",
    "\n",
    "    def call_api(self, prompt: str, system_prompt: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        # Sleep a random time to simulate latency\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "        # Return a pretend success payload\n",
    "        return {\n",
    "            \"content\": f\"Fake response for prompt: {prompt[:30]}...\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def validate_response(self, response: Dict[str, Any]) -> bool:\n",
    "        return response.get(\"success\", False)\n",
    "\n",
    "########################################\n",
    "# 2) Generate a Synthetic Dataset\n",
    "########################################\n",
    "\n",
    "def generate_fake_dataset(num_items: int = 2000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of dicts with random text data.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(num_items):\n",
    "        random_text = ''.join(random.choices(string.ascii_lowercase + ' ', k=100))\n",
    "        item = {\n",
    "            \"id\": i,\n",
    "            \"text\": random_text\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "########################################\n",
    "# 3) The \"process function\" for item-level\n",
    "########################################\n",
    "\n",
    "def process_item_with_fake_llm(item: Dict[str, Any], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given a single item, call the fake LLM API and return a small result dict.\n",
    "    \"\"\"\n",
    "    prompt = f\"Process text: {item['text']}\"\n",
    "    response = client.call_api(prompt)\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"content\": response[\"content\"],  # from the fake LLM\n",
    "        \"success\": response[\"success\"]\n",
    "    }\n",
    "\n",
    "########################################\n",
    "# 4) No concurrency: control group\n",
    "########################################\n",
    "\n",
    "def run_sequential_no_concurrency(dataset, client):\n",
    "    \"\"\"\n",
    "    Control group: process each item in a simple for-loop (sequential).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Sequential (control)\"):\n",
    "        prompt = f\"Process text: {item['text']}\"\n",
    "        response = client.call_api(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"content\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        })\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Control] Processed {len(results)} items sequentially in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 5) The Library with item-level concurrency\n",
    "########################################\n",
    "\n",
    "from multi_processing.processor import LLMProcessor\n",
    "from multi_processing.processor_config import ProcessorConfig\n",
    "\n",
    "def run_with_library(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use your LLMProcessor with concurrency, no caching, etc., item-level.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=1,         # item-level concurrency\n",
    "        fail_fast=False,\n",
    "        enable_batch_prompts=False  # <== ensure item-level\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(item):\n",
    "        return process_item_with_fake_llm(item, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    results = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\", \n",
    "        use_cache=False\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Library Item-Level] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 6) The Library with \"Batch\" prompts\n",
    "########################################\n",
    "\n",
    "def create_subbatch_prompt(items: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Example: combine multiple items into a single prompt.\n",
    "    items might be up to config.batch_size in length.\n",
    "    \"\"\"\n",
    "    # Just build a single text listing them\n",
    "    combined_text = \"\"\n",
    "    for itm in items:\n",
    "        combined_text += f\"(ID={itm['id']}) {itm['text']}\\n\"\n",
    "    prompt = f\"Process these {len(items)} texts at once:\\n{combined_text}\"\n",
    "    return prompt\n",
    "\n",
    "def process_subbatch(subbatch: List[Dict[str, Any]], client: BaseLLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Called once per sub-batch. We call the LLM once for all items in subbatch.\n",
    "    Return a dict with the results. For example, { item_id -> info }.\n",
    "    \"\"\"\n",
    "    prompt = create_subbatch_prompt(subbatch)\n",
    "    response = client.call_api(prompt)\n",
    "    \n",
    "    # We'll pretend we parse out something. For now, just store the prompt.\n",
    "    # In a real scenario, you'd parse JSON containing all items' results.\n",
    "    result_map = {}\n",
    "    for itm in subbatch:\n",
    "        result_map[itm['id']] = {\n",
    "            \"id\": itm[\"id\"],\n",
    "            \"text\": itm[\"text\"],\n",
    "            \"combined_response\": response[\"content\"],\n",
    "            \"success\": response[\"success\"]\n",
    "        }\n",
    "    return result_map\n",
    "\n",
    "def run_with_library_batch_mode(dataset: List[Dict[str, Any]], max_workers: int = 10, batch_size: int = 10):\n",
    "    \"\"\"\n",
    "    Concurrency across sub-batches (enable_batch_prompts=True).\n",
    "    Each sub-batch calls the LLM once for multiple items.\n",
    "    \"\"\"\n",
    "    config = ProcessorConfig(\n",
    "        cache_enabled=False,\n",
    "        max_workers=max_workers,\n",
    "        rate_limit=0.0,\n",
    "        max_retries=1,\n",
    "        batch_size=batch_size,  # sub-batch size\n",
    "        fail_fast=False,\n",
    "        enable_batch_prompts=True  # <== batch mode\n",
    "    )\n",
    "    \n",
    "    client = FakeLLMClient()\n",
    "    processor = LLMProcessor(llm_client=client, config=config)\n",
    "\n",
    "    def process_fn(subbatch: List[Dict[str, Any]]) -> Dict[int, Any]:\n",
    "        return process_subbatch(subbatch, client)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    # This returns a list of dicts. Each dict is { item_id -> info } for one sub-batch\n",
    "    dict_list = processor.process_batch(\n",
    "        items=dataset,\n",
    "        process_fn=process_fn,\n",
    "        cache_prefix=\"\", \n",
    "        use_cache=False,\n",
    "        desc=\"Library Batch Mode\"\n",
    "    )\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "\n",
    "    # Combine all subdicts\n",
    "    combined = {}\n",
    "    for subdict in dict_list:\n",
    "        combined.update(subdict)  # merges item_id -> info\n",
    "    \n",
    "    print(f\"[Library Batch Mode] Processed {len(combined)} items in {elapsed:.2f} seconds (sub-batch size={batch_size}).\")\n",
    "    return combined\n",
    "\n",
    "########################################\n",
    "# 7) Raw Concurrency (ThreadPool)\n",
    "########################################\n",
    "\n",
    "def run_with_raw_concurrency(dataset: List[Dict[str, Any]], max_workers: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use a plain ThreadPoolExecutor approach, no library overhead.\n",
    "    \"\"\"\n",
    "    client = FakeLLMClient()\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=\"Raw concurrency\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in dataset:\n",
    "                futures.append(executor.submit(process_item_with_fake_llm, item, client))\n",
    "            \n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                res = f.result()\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[Raw Concurrency] Processed {len(results)} items in {elapsed:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# 8) Main Comparison\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate a sample dataset\n",
    "    data_size = 500\n",
    "    dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "    # We'll re-use one FakeLLMClient for the control\n",
    "    control_client = FakeLLMClient()\n",
    "    \n",
    "    # 2) Control: No concurrency\n",
    "    control_results = run_sequential_no_concurrency(dataset, control_client)\n",
    "\n",
    "    # 3) Library, item-level concurrency\n",
    "    library_item_results = run_with_library(dataset, max_workers=100)\n",
    "\n",
    "    # 4) Library, batch mode concurrency\n",
    "    #    Each sub-batch processes multiple items in a single fake API call\n",
    "    library_batch_results = run_with_library_batch_mode(dataset, max_workers=100, batch_size=10)\n",
    "\n",
    "    # 5) Raw concurrency\n",
    "    raw_results = run_with_raw_concurrency(dataset, max_workers=100)\n",
    "\n",
    "    # 6) Print final comparisons\n",
    "    print(f\"\\nControl results: {len(control_results)} items.\")\n",
    "    print(f\"Library item-level results: {len(library_item_results)} items.\")\n",
    "    print(f\"Library batch-mode results: {len(library_batch_results)} items.\")\n",
    "    print(f\"Raw concurrency results: {len(raw_results)} items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 61390\n"
     ]
    }
   ],
   "source": [
    "data_size = 500\n",
    "dataset = generate_fake_dataset(num_items=data_size)\n",
    "\n",
    "def count_tokens(items: list) -> int:\n",
    "    sum = 0\n",
    "    for item in items:\n",
    "        # Since dataset is a list of dictionaries, we can directly get the length of each item\n",
    "        sum += len(str(item))  # Convert item to string to get its length\n",
    "    return sum\n",
    "\n",
    "print(f\"Total tokens: {count_tokens(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "dataset.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
